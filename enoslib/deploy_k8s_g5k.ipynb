{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6f19e1",
   "metadata": {},
   "source": [
    "# Importations et Configuration Initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60918c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING]: failed to patch stdout/stderr for fork-safety: 'OutStream' object\n",
      "has no attribute 'buffer'\n",
      "[WARNING]: failed to reconfigure stdout/stderr with custom encoding error\n",
      "handler: 'OutStream' object has no attribute 'reconfigure'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration initiale chargée.\n"
     ]
    }
   ],
   "source": [
    "import enoslib as en\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# --- Configuration de l'expérience ---\n",
    "\n",
    "# Configurez le logging pour avoir un retour clair sur les étapes\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# MODIFIEZ CETTE LIGNE AVEC VOTRE NOM D'UTILISATEUR GRID'5000\n",
    "G5K_USER = \"wemenra\"\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "G5K_SITE = \"rennes\"  # Site Grid'5000 à utiliser\n",
    "\n",
    "# Nom du job pour la réservation sur Grid'5000\n",
    "JOB_NAME = \"K8sEnoslibDeploy\"\n",
    "\n",
    "# Clé SSH, par défaut enoslib cherche ~/.ssh/id_rsa\n",
    "# Si vous en utilisez une autre, décommentez et modifiez la ligne suivante :\n",
    "# en.set_config(ssh_key=\"~/.ssh/ma_cle_ssh\")\n",
    "\n",
    "# Création du chemin pour les sorties (par exemple, inventaire)\n",
    "Path(\"inventory\").mkdir(exist_ok=True)\n",
    "OUTPUT_INVENTORY = \"inventory/g5k_inventory.yaml\"\n",
    "\n",
    "print(\"Configuration initiale chargée.\")\n",
    "if G5K_USER == \"VOTRE_LOGIN_G5K\":\n",
    "    print(\"\\n⚠️  ATTENTION: N'oubliez pas de remplacer 'VOTRE_LOGIN_G5K' par votre nom d'utilisateur Grid'5000.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db030d",
   "metadata": {},
   "source": [
    "# Définition des Ressources pour le Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89826ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Définition des ressources terminée.\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 2\n",
    "# Définition de la configuration pour la connexion à Grid'5000\n",
    "# en.set_provider(\n",
    "#     en.G5k(\n",
    "#         username=G5K_USER,\n",
    "#         site=G5K_SITE,\n",
    "#         job_name=JOB_NAME,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Définition des ressources à réserver\n",
    "# 1 nœud master et 2 nœuds workers\n",
    "# Tous les nœuds seront déployés avec Ubuntu 22.04\n",
    "conf = (\n",
    "en.G5kConf.from_settings(\n",
    "    job_name=JOB_NAME,\n",
    "    job_type=[\"deploy\"],\n",
    "    walltime=\"03:30:00\", # Réservez pour 1 heure\n",
    "    env_name=\"ubuntu2204-min\"\n",
    ").add_machine(\n",
    "    roles=[\"master\"],\n",
    "    cluster=\"paradoxe\",\n",
    "    nodes=1\n",
    ").add_machine(\n",
    "    roles=[\"workers\"],\n",
    "    cluster=\"paradoxe\",\n",
    "    nodes=2\n",
    ")\n",
    ")\n",
    "\n",
    "print(\"Définition des ressources terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7476000e",
   "metadata": {},
   "source": [
    "# Exécution de la Réservation et du Déploiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0123a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:enoslib.log:[G5k] Submitting {'name': 'K8sEnoslibDeploy', 'types': ['deploy'], 'resources': \"{cluster='paradoxe'}/nodes=1+{cluster='paradoxe'}/nodes=2,walltime=03:30:00\", 'command': 'sleep 31536000', 'queue': 'default'} on rennes\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:49:02]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:50:20]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:51:39]\n",
      "INFO:enoslib.log:[G5k] Waiting for 3292757 on rennes [2025-10-08 12:52:36]\n",
      "INFO:enoslib.log:[G5k] All jobs are Running !\n",
      "INFO:enoslib.log:[G5k] Deploy the public key contained in /home/romial/.ssh/id_rsa.pub to remote hosts.\n",
      "INFO:enoslib.log:[G5k] Deploying ['paradoxe-7.rennes.grid5000.fr', 'paradoxe-8.rennes.grid5000.fr', 'paradoxe-9.rennes.grid5000.fr'] with options {'environment': 'ubuntu2204-min', 'key': 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCspk0GLFN8ZdmvEinnrN7rLIOSMvilTypm+ZR5zTzdol5T/5g/vCT1U+jBOaP9y/SCw31+4KOoZoKNQewAdEh7l+5FnfJL2VD1XaLOCo+l6aoNj17nLFeiRvesNYPwPCll5vN6mfA1CDXIhBcEfuJC0wmIM9c6Qz/WFcqhpbdtpjVsz0h+bqX8+45M4Ek4JB55xQL0h1PUK+wrjhyeFPzgq2uEeeHrBAqf4EyQuv6qSkSHcT39HdUnDkJMrO4U5KMLxMbDB6iqPvRTOb7nOSxT0FF71wYRWNLKdqqtMBI0tF6IUiwc0xau0lgNDFUIXEdJCfIrljKRkWZ5mcEienAkq3/1BSW6qNb8NxzGkAkeyPDuHXlwic4guSC6NWt+xasT+Dqr7KlkealL3eKQbf3A3l7Luov4kCz9gWqVlcDnoMsQSg1mpVBkqng4+Udn+bTl4cuwwHjfd+32pjW/wfiVI/X8OXyaWNuUcVRt2EofA2wPmcIdc9PuLjKkB5Bl6lE= romial@PL-DAPI-NA-SALSA-004\\n'} [1/3]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n",
      "INFO:enoslib.log:[G5k] Waiting for the end of deployment [D-c09b43cf-1268-40f6-b49f-43d2c166cced]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f166f6548f47ab97d87302529b94c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">Run dhcp on the nodes</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-9.rennes.grid5000.fr'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-8.rennes.grid5000.fr'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mRun dhcp on the nodes\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-9.rennes.grid5000.fr'\u001b[0m, \n",
       "\u001b[32m'paradoxe-8.rennes.grid5000.fr'\u001b[0m, \u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Réservation et déploiement terminés ! ---\n",
      "Master: ['paradoxe-7.rennes.grid5000.fr']\n",
      "Workers: ['paradoxe-9.rennes.grid5000.fr', 'paradoxe-8.rennes.grid5000.fr']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "provider = en.G5k(conf)\n",
    "# Démarrage de la réservation et du déploiement\n",
    "# provider.init() bloque jusqu'à ce que les noeuds soient prêts\n",
    "\n",
    "roles, networks = provider.init()\n",
    "\n",
    "# Récupération des informations sur les rôles pour les cellules suivantes\n",
    "#roles = reservation.get_roles()\n",
    "master_node = roles[\"master\"]\n",
    "worker_nodes = roles[\"workers\"]\n",
    "\n",
    "print(\"--- Réservation et déploiement terminés ! ---\")\n",
    "print(f\"Master: {[node.address for node in master_node]}\")\n",
    "print(f\"Workers: {[node.address for node in worker_nodes]}\")\n",
    "#print(f\"Tous les nœuds: roles\" + str(roles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f16248",
   "metadata": {},
   "source": [
    "#  Préparation Commune de Tous les Nœuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f110227d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdac6328cef2479a9cec82e35ed1c5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de la préparation commune...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-9.rennes.grid5000.fr'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-8.rennes.grid5000.fr'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-9.rennes.grid5000.fr'\u001b[0m, \n",
       "\u001b[32m'paradoxe-8.rennes.grid5000.fr'\u001b[0m, \u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préparation commune terminée.\n"
     ]
    }
   ],
   "source": [
    "# Script de configuration commun\n",
    "COMMON_SETUP_SCRIPT = \"\"\"\n",
    "#!/bin/bash -xe\n",
    "sudo apt-get update -y\n",
    "sudo apt-get install -y software-properties-common gpg curl apt-transport-https ca-certificates jq\n",
    "cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf\n",
    "overlay\n",
    "br_netfilter\n",
    "EOF\n",
    "sudo modprobe overlay\n",
    "sudo modprobe br_netfilter\n",
    "cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\n",
    "net.bridge.bridge-nf-call-iptables  = 1\n",
    "net.bridge.bridge-nf-call-ip6tables = 1\n",
    "net.ipv4.ip_forward                 = 1\n",
    "EOF\n",
    "sudo sysctl --system\n",
    "sudo swapoff -a\n",
    "curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key | gpg --dearmor | sudo tee /etc/apt/keyrings/cri-o-apt-keyring.gpg >/dev/null\n",
    "echo \"deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /\" | sudo tee /etc/apt/sources.list.d/cri-o.list\n",
    "sudo apt-get update -y\n",
    "sudo apt-get install -y cri-o\n",
    "sudo systemctl enable --now crio\n",
    "KUBERNETES_VERSION=\"1.30\"\n",
    "sudo mkdir -p /etc/apt/keyrings\n",
    "curl -fsSL https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n",
    "echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/ /\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y kubelet=1.30.0-1.1 kubectl=1.30.0-1.1 kubeadm=1.30.0-1.1\n",
    "sudo apt-mark hold kubelet kubeadm kubectl\n",
    "IFACE=$(ip route | grep default | awk '{print $5}')\n",
    "local_ip=$(ip --json addr show $IFACE | jq -r '.[0].addr_info[] | select(.family == \"inet\") | .local')\n",
    "echo \"KUBELET_EXTRA_ARGS=--node-ip=$local_ip\" | sudo tee /etc/default/kubelet\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script sur tous les nœuds\n",
    "print(\"Lancement de la préparation commune...\")\n",
    "with en.actions(roles=roles) as p:\n",
    "    p.shell(COMMON_SETUP_SCRIPT)\n",
    "print(\"Préparation commune terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18b04e",
   "metadata": {},
   "source": [
    "##  Vérification de la Préparation des Nœuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f78df3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fb52effc2f42a5bd11e4fcecd21988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de la vérification sur tous les nœuds...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-9.rennes.grid5000.fr'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-8.rennes.grid5000.fr'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-9.rennes.grid5000.fr'\u001b[0m, \n",
       "\u001b[32m'paradoxe-8.rennes.grid5000.fr'\u001b[0m, \u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Vérification de l'état des nœuds ---\n",
      "[VÉRIF] Service CRI-O (runtime de conteneurs)...\n",
      "✅ OK: Le service CRI-O est actif.\n",
      "[VÉRIF] Binaires Kubernetes...\n",
      "Kubernetes v1.30.0\n",
      "kubeadm version: &version.Info{Major:\"1\", Minor:\"30\", GitVersion:\"v1.30.0\", GitCommit:\"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a\", GitTreeState:\"clean\", BuildDate:\"2024-04-17T17:34:08Z\", GoVersion:\"go1.22.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n",
      "✅ OK: Les binaires Kubelet et Kubeadm sont installés.\n",
      "[VÉRIF] Modules Kernel...\n",
      "✅ OK: Module 'br_netfilter' est chargé.\n",
      "✅ OK: Module 'overlay' est chargé.\n",
      "\n",
      "🎉 La préparation de ce nœud est validée.\n"
     ]
    }
   ],
   "source": [
    "# Script de vérification (inchangé)\n",
    "VERIFY_PREP_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "echo \"--- Vérification de l'état des nœuds ---\"\n",
    "echo \"[VÉRIF] Service CRI-O (runtime de conteneurs)...\"\n",
    "sudo systemctl is-active --quiet crio\n",
    "echo \"✅ OK: Le service CRI-O est actif.\"\n",
    "echo \"[VÉRIF] Binaires Kubernetes...\"\n",
    "kubelet --version\n",
    "kubeadm version\n",
    "echo \"✅ OK: Les binaires Kubelet et Kubeadm sont installés.\"\n",
    "echo \"[VÉRIF] Modules Kernel...\"\n",
    "lsmod | grep -q br_netfilter && echo \"✅ OK: Module 'br_netfilter' est chargé.\"\n",
    "lsmod | grep -q overlay && echo \"✅ OK: Module 'overlay' est chargé.\"\n",
    "echo \"\\n🎉 La préparation de ce nœud est validée.\"\n",
    "\"\"\"\n",
    "\n",
    "# 1. On exécute les actions (comme avant)\n",
    "print(\"Lancement de la vérification sur tous les nœuds...\")\n",
    "with en.actions(roles=roles) as p:\n",
    "    p.shell(VERIFY_PREP_SCRIPT)\n",
    "\n",
    "results = p.results\n",
    "verify = results[0].stdout.strip()\n",
    "print(verify)\n",
    "# # 2. ✅ NOUVEAU : On récupère et on affiche les résultats APRÈS le bloc\n",
    "# print(\"\\n--- RÉSULTATS DE LA VÉRIFICATION ---\")\n",
    "# results = p.get_results()\n",
    "\n",
    "# # On boucle sur les résultats de chaque machine\n",
    "# for r in results:\n",
    "#     print(f\"\\n======== Nœud: {r.host} ========\")\n",
    "#     print(r.stdout) # Affiche la sortie standard de la commande\n",
    "    \n",
    "#     # Optionnel : Affiche les erreurs s'il y en a\n",
    "#     if r.stderr:\n",
    "#         print(f\"-------- Erreurs (stderr) pour {r.host} --------\")\n",
    "#         print(r.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3926281",
   "metadata": {},
   "source": [
    "#  Initialisation du Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddc5e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf4d4d1276e46618fd8d0a5823b3885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_1875005/3805460087.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  MASTER_INIT_SCRIPT = \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation du master...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation du master terminée.\n"
     ]
    }
   ],
   "source": [
    "# Script d'initialisation du master\n",
    "MASTER_INIT_SCRIPT = \"\"\"\n",
    "#!/bin/bash -xe\n",
    "IFACE=$(ip route | grep default | awk '{print $5}')\n",
    "IPADDR=$(ip -4 addr show $IFACE | grep -oP '(?<=inet\\s)\\d+(\\.\\d+){3}')\n",
    "NODENAME=$(hostname -s)\n",
    "sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --apiserver-advertise-address=$IPADDR --node-name $NODENAME --cri-socket unix:///var/run/crio/crio.sock --ignore-preflight-errors=Swap\n",
    "mkdir -p $HOME/.kube\n",
    "sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n",
    "sudo chown $(id -u):$(id -g) $HOME/.kube/config\n",
    "kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script sur le master\n",
    "print(\"Initialisation du master...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(MASTER_INIT_SCRIPT)\n",
    "print(\"Initialisation du master terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079e4de",
   "metadata": {},
   "source": [
    "# Jonction des Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278b9b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee482125ee3492a94a859e1973f7b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération de la commande de jonction depuis le master...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca0ace45b2f4777a4d7cc79a9131c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Commande de jonction récupérée: kubeadm join 172.16.101.7:6443 --token ss230c.9tuuf3alqmrojb03 --discovery-token-ca-cert-hash sha256:d5cb0276f85673c213dda06080f7e9ef98816b7eb2ce35039cf888eb1d58438a\n",
      "Les workers rejoignent le cluster...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-9.rennes.grid5000.fr'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-8.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-9.rennes.grid5000.fr'\u001b[0m, \n",
       "\u001b[32m'paradoxe-8.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workers joints. Attente de 60s pour la stabilisation...\n"
     ]
    }
   ],
   "source": [
    "# 1. Récupérer la commande de jonction depuis le master\n",
    "print(\"Récupération de la commande de jonction depuis le master...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(\"sudo kubeadm token create --print-join-command\")\n",
    "\n",
    "# Récupération des résultats après le bloc\n",
    "results = p.results\n",
    "join_command = results[0].stdout.strip()\n",
    "print(f\"✅ Commande de jonction récupérée: {join_command}\")\n",
    "\n",
    "# 2. Exécuter la commande sur les workers\n",
    "print(\"Les workers rejoignent le cluster...\")\n",
    "with en.actions(roles=worker_nodes) as p:\n",
    "    p.shell(f\"sudo {join_command}\")\n",
    "\n",
    "print(\"Workers joints. Attente de 60s pour la stabilisation...\")\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a921f89",
   "metadata": {},
   "source": [
    "# Vérification du Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7195aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c80d35083564c72beecde09b98a5041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- État du cluster ---\n",
      "NAME         STATUS   ROLES           AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME\n",
      "paradoxe-7   Ready    control-plane   2m16s   v1.30.0   172.16.101.7   <none>        Ubuntu 22.04.5 LTS   5.15.0-144-generic   cri-o://1.33.0\n",
      "paradoxe-8   Ready    <none>          70s     v1.30.0   172.16.101.8   <none>        Ubuntu 22.04.5 LTS   5.15.0-144-generic   cri-o://1.33.0\n",
      "paradoxe-9   Ready    <none>          70s     v1.30.0   172.16.101.9   <none>        Ubuntu 22.04.5 LTS   5.15.0-144-generic   cri-o://1.33.0\n"
     ]
    }
   ],
   "source": [
    "# Exécution de kubectl get nodes sur le master pour vérifier\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(\"kubectl get nodes -o wide\")\n",
    "\n",
    "results = p.results\n",
    "nodes = results[0].stdout.strip()\n",
    "print(\"--- État du cluster ---\")\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524c748",
   "metadata": {},
   "source": [
    " ## Installation des Add-ons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c717bd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9daa0d143046518c26bcf192c366e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation des add-ons sur le nœud master...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Résultat de l'installation ---\n",
      "--- Installation du Metrics Server ---\n",
      "serviceaccount/metrics-server created\n",
      "clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\n",
      "clusterrole.rbac.authorization.k8s.io/system:metrics-server created\n",
      "rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\n",
      "service/metrics-server created\n",
      "deployment.apps/metrics-server created\n",
      "apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n",
      "\n",
      "--- Installation d'Ingress-NGINX ---\n",
      "namespace/ingress-nginx created\n",
      "serviceaccount/ingress-nginx created\n",
      "serviceaccount/ingress-nginx-admission created\n",
      "role.rbac.authorization.k8s.io/ingress-nginx created\n",
      "role.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "clusterrole.rbac.authorization.k8s.io/ingress-nginx created\n",
      "clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "rolebinding.rbac.authorization.k8s.io/ingress-nginx created\n",
      "rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "configmap/ingress-nginx-controller created\n",
      "service/ingress-nginx-controller created\n",
      "service/ingress-nginx-controller-admission created\n",
      "deployment.apps/ingress-nginx-controller created\n",
      "job.batch/ingress-nginx-admission-create created\n",
      "job.batch/ingress-nginx-admission-patch created\n",
      "ingressclass.networking.k8s.io/nginx created\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created\n",
      "\n",
      "Installation des add-ons terminée.\n"
     ]
    }
   ],
   "source": [
    "# Script pour installer les add-ons\n",
    "ADDONS_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- Installation du Metrics Server ---\"\n",
    "kubectl apply -f https://raw.githubusercontent.com/techiescamp/kubeadm-scripts/main/manifests/metrics-server.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Installation d'Ingress-NGINX ---\"\n",
    "kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.13.0/deploy/static/provider/baremetal/deploy.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"Installation des add-ons terminée.\"\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script sur le master\n",
    "print(\"Installation des add-ons sur le nœud master...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(ADDONS_SCRIPT)\n",
    "\n",
    "# Récupération de la sortie comme vous l'avez spécifié\n",
    "print(\"\\n--- Résultat de l'installation ---\")\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"Aucun résultat à afficher. Vérifiez les erreurs potentielles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b258c",
   "metadata": {},
   "source": [
    "# Déploiement et Test d'une Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ab194c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37246a3ef2d442f89c9f76df8a776be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Déploiement de l'application de test...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Résultat du déploiement ---\n",
      "--- Création du Deployment et du Service pour l'application echoserver ---\n",
      "deployment.apps/echoserver created\n",
      "service/echoserver created\n",
      "\n",
      "--- Attente de 15 secondes pour le démarrage des pods ---\n",
      "\n",
      "--- Vérification du statut du déploiement ---\n",
      "NAME         READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "echoserver   2/2     2            2           15s\n",
      "\n",
      "--- Vérification des pods (devrait en afficher 2) ---\n",
      "NAME                         READY   STATUS    RESTARTS   AGE\n",
      "echoserver-d75ff78c5-2vrcq   1/1     Running   0          15s\n",
      "echoserver-d75ff78c5-g5vtf   1/1     Running   0          15s\n",
      "\n",
      "--- Vérification du service (notez le port après '80:') ---\n",
      "NAME         TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\n",
      "echoserver   NodePort   10.111.38.19   <none>        80:32455/TCP   15s\n"
     ]
    }
   ],
   "source": [
    "# Script pour déployer l'application de test et son service\n",
    "APP_DEPLOY_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- Création du Deployment et du Service pour l'application echoserver ---\"\n",
    "\n",
    "# On utilise un 'heredoc' pour passer le YAML directement à kubectl\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: echoserver\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: echoserver\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: echoserver\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: echoserver\n",
    "        image: registry.k8s.io/echoserver:1.10\n",
    "        ports:\n",
    "        - containerPort: 8080\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: echoserver\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    app: echoserver\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080\n",
    "EOF\n",
    "\n",
    "echo \"\\n--- Attente de 15 secondes pour le démarrage des pods ---\"\n",
    "sleep 15\n",
    "\n",
    "echo \"\\n--- Vérification du statut du déploiement ---\"\n",
    "kubectl get deployment echoserver\n",
    "\n",
    "echo \"\\n--- Vérification des pods (devrait en afficher 2) ---\"\n",
    "kubectl get pods -l app=echoserver\n",
    "\n",
    "echo \"\\n--- Vérification du service (notez le port après '80:') ---\"\n",
    "kubectl get service echoserver\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script sur le master\n",
    "print(\"Déploiement de l'application de test...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(APP_DEPLOY_SCRIPT)\n",
    "\n",
    "# Affichage du résultat\n",
    "print(\"\\n--- Résultat du déploiement ---\")\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c83c8",
   "metadata": {},
   "source": [
    "### Envoyez une Requête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "558e8487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0e5c70e1bc442ea4ba4dee17e1db42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Réponse du serveur ---\n",
      "\n",
      "\n",
      "Hostname: echoserver-d75ff78c5-g5vtf\n",
      "\n",
      "Pod Information:\n",
      "\t-no pod information available-\n",
      "\n",
      "Server values:\n",
      "\tserver_version=nginx: 1.13.3 - lua: 10008\n",
      "\n",
      "Request Information:\n",
      "\tclient_address=192.168.55.0\n",
      "\tmethod=GET\n",
      "\treal path=/\n",
      "\tquery=\n",
      "\trequest_version=1.1\n",
      "\trequest_scheme=http\n",
      "\trequest_uri=http://paradoxe-7.rennes.grid5000.fr:8080/\n",
      "\n",
      "Request Headers:\n",
      "\taccept=*/*\n",
      "\thost=paradoxe-7.rennes.grid5000.fr:32455\n",
      "\tuser-agent=curl/7.81.0\n",
      "\n",
      "Request Body:\n",
      "\t-no body in request-\n"
     ]
    }
   ],
   "source": [
    "# Récupère l'adresse IP du master\n",
    "master_ip = master_node[0].address \n",
    "\n",
    "# Remplacez 31192 par le port que vous avez obtenu à l'étape 1\n",
    "node_port = 32455 # ⚠️ CHANGEZ CE PORT\n",
    "\n",
    "# Test avec curl depuis le master\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(f\"curl http://{master_ip}:{node_port}\")\n",
    "\n",
    "if p.results:\n",
    "    print(\"--- Réponse du serveur ---\")\n",
    "    print(p.results[0].stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4fcd5b",
   "metadata": {},
   "source": [
    "# Installation de la Suite de Monitoring (Prometheus & Grafana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad67271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c158fb6ea5be49e38fed72552df006f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de l'installation de la suite de monitoring...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Résultat de l'installation ---\n",
      "--- Installation de la suite Prometheus + Grafana via kube-prometheus ---\n",
      "Clonage du dépôt kube-prometheus...\n",
      "Étape 1/3 : Application des CRDs et de la configuration 'setup'...\n",
      "customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/prometheusagents.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/scrapeconfigs.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com serverside-applied\n",
      "customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com serverside-applied\n",
      "namespace/monitoring serverside-applied\n",
      "Attente de l'établissement des CRDs...\n",
      "customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/prometheusagents.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/scrapeconfigs.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com condition met\n",
      "customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com condition met\n",
      "Étape 2/3 : Application des manifestes de la suite de monitoring...\n",
      "alertmanager.monitoring.coreos.com/main created\n",
      "networkpolicy.networking.k8s.io/alertmanager-main created\n",
      "poddisruptionbudget.policy/alertmanager-main created\n",
      "prometheusrule.monitoring.coreos.com/alertmanager-main-rules created\n",
      "secret/alertmanager-main created\n",
      "service/alertmanager-main created\n",
      "serviceaccount/alertmanager-main created\n",
      "servicemonitor.monitoring.coreos.com/alertmanager-main created\n",
      "clusterrole.rbac.authorization.k8s.io/blackbox-exporter created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/blackbox-exporter created\n",
      "configmap/blackbox-exporter-configuration created\n",
      "deployment.apps/blackbox-exporter created\n",
      "networkpolicy.networking.k8s.io/blackbox-exporter created\n",
      "service/blackbox-exporter created\n",
      "serviceaccount/blackbox-exporter created\n",
      "servicemonitor.monitoring.coreos.com/blackbox-exporter created\n",
      "secret/grafana-config created\n",
      "secret/grafana-datasources created\n",
      "configmap/grafana-dashboard-alertmanager-overview created\n",
      "configmap/grafana-dashboard-apiserver created\n",
      "configmap/grafana-dashboard-cluster-total created\n",
      "configmap/grafana-dashboard-controller-manager created\n",
      "configmap/grafana-dashboard-grafana-overview created\n",
      "configmap/grafana-dashboard-k8s-resources-cluster created\n",
      "configmap/grafana-dashboard-k8s-resources-multicluster created\n",
      "configmap/grafana-dashboard-k8s-resources-namespace created\n",
      "configmap/grafana-dashboard-k8s-resources-node created\n",
      "configmap/grafana-dashboard-k8s-resources-pod created\n",
      "configmap/grafana-dashboard-k8s-resources-windows-cluster created\n",
      "configmap/grafana-dashboard-k8s-resources-windows-namespace created\n",
      "configmap/grafana-dashboard-k8s-resources-windows-pod created\n",
      "configmap/grafana-dashboard-k8s-resources-workload created\n",
      "configmap/grafana-dashboard-k8s-resources-workloads-namespace created\n",
      "configmap/grafana-dashboard-k8s-windows-cluster-rsrc-use created\n",
      "configmap/grafana-dashboard-k8s-windows-node-rsrc-use created\n",
      "configmap/grafana-dashboard-kubelet created\n",
      "configmap/grafana-dashboard-namespace-by-pod created\n",
      "configmap/grafana-dashboard-namespace-by-workload created\n",
      "configmap/grafana-dashboard-node-cluster-rsrc-use created\n",
      "configmap/grafana-dashboard-node-rsrc-use created\n",
      "configmap/grafana-dashboard-nodes-aix created\n",
      "configmap/grafana-dashboard-nodes-darwin created\n",
      "configmap/grafana-dashboard-nodes created\n",
      "configmap/grafana-dashboard-persistentvolumesusage created\n",
      "configmap/grafana-dashboard-pod-total created\n",
      "configmap/grafana-dashboard-prometheus-remote-write created\n",
      "configmap/grafana-dashboard-prometheus created\n",
      "configmap/grafana-dashboard-proxy created\n",
      "configmap/grafana-dashboard-scheduler created\n",
      "configmap/grafana-dashboard-workload-total created\n",
      "configmap/grafana-dashboards created\n",
      "deployment.apps/grafana created\n",
      "networkpolicy.networking.k8s.io/grafana created\n",
      "prometheusrule.monitoring.coreos.com/grafana-rules created\n",
      "service/grafana created\n",
      "serviceaccount/grafana created\n",
      "servicemonitor.monitoring.coreos.com/grafana created\n",
      "prometheusrule.monitoring.coreos.com/kube-prometheus-rules created\n",
      "clusterrole.rbac.authorization.k8s.io/kube-state-metrics created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created\n",
      "deployment.apps/kube-state-metrics created\n",
      "networkpolicy.networking.k8s.io/kube-state-metrics created\n",
      "prometheusrule.monitoring.coreos.com/kube-state-metrics-rules created\n",
      "service/kube-state-metrics created\n",
      "serviceaccount/kube-state-metrics created\n",
      "servicemonitor.monitoring.coreos.com/kube-state-metrics created\n",
      "prometheusrule.monitoring.coreos.com/kubernetes-monitoring-rules created\n",
      "servicemonitor.monitoring.coreos.com/kube-apiserver created\n",
      "servicemonitor.monitoring.coreos.com/coredns created\n",
      "servicemonitor.monitoring.coreos.com/kube-controller-manager created\n",
      "servicemonitor.monitoring.coreos.com/kube-scheduler created\n",
      "servicemonitor.monitoring.coreos.com/kubelet created\n",
      "clusterrole.rbac.authorization.k8s.io/node-exporter created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/node-exporter created\n",
      "daemonset.apps/node-exporter created\n",
      "networkpolicy.networking.k8s.io/node-exporter created\n",
      "prometheusrule.monitoring.coreos.com/node-exporter-rules created\n",
      "service/node-exporter created\n",
      "serviceaccount/node-exporter created\n",
      "servicemonitor.monitoring.coreos.com/node-exporter created\n",
      "clusterrole.rbac.authorization.k8s.io/prometheus-k8s created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created\n",
      "networkpolicy.networking.k8s.io/prometheus-k8s created\n",
      "poddisruptionbudget.policy/prometheus-k8s created\n",
      "prometheus.monitoring.coreos.com/k8s created\n",
      "prometheusrule.monitoring.coreos.com/prometheus-k8s-prometheus-rules created\n",
      "rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created\n",
      "rolebinding.rbac.authorization.k8s.io/prometheus-k8s created\n",
      "rolebinding.rbac.authorization.k8s.io/prometheus-k8s created\n",
      "rolebinding.rbac.authorization.k8s.io/prometheus-k8s created\n",
      "role.rbac.authorization.k8s.io/prometheus-k8s-config created\n",
      "role.rbac.authorization.k8s.io/prometheus-k8s created\n",
      "role.rbac.authorization.k8s.io/prometheus-k8s created\n",
      "role.rbac.authorization.k8s.io/prometheus-k8s created\n",
      "service/prometheus-k8s created\n",
      "serviceaccount/prometheus-k8s created\n",
      "servicemonitor.monitoring.coreos.com/prometheus-k8s created\n",
      "apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io configured\n",
      "clusterrole.rbac.authorization.k8s.io/prometheus-adapter created\n",
      "clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader configured\n",
      "clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created\n",
      "clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created\n",
      "configmap/adapter-config created\n",
      "deployment.apps/prometheus-adapter created\n",
      "networkpolicy.networking.k8s.io/prometheus-adapter created\n",
      "poddisruptionbudget.policy/prometheus-adapter created\n",
      "rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created\n",
      "service/prometheus-adapter created\n",
      "serviceaccount/prometheus-adapter created\n",
      "servicemonitor.monitoring.coreos.com/prometheus-adapter created\n",
      "clusterrole.rbac.authorization.k8s.io/prometheus-operator created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created\n",
      "deployment.apps/prometheus-operator created\n",
      "networkpolicy.networking.k8s.io/prometheus-operator created\n",
      "prometheusrule.monitoring.coreos.com/prometheus-operator-rules created\n",
      "service/prometheus-operator created\n",
      "serviceaccount/prometheus-operator created\n",
      "servicemonitor.monitoring.coreos.com/prometheus-operator created\n",
      "Étape 3/3 : Attente du démarrage de Prometheus et Grafana (peut prendre quelques minutes)...\n",
      "deployment.apps/grafana condition met\n",
      "\n",
      "--- Vérification des pods dans le namespace 'monitoring' ---\n",
      "NAME                                   READY   STATUS            RESTARTS   AGE\n",
      "alertmanager-main-0                    1/2     Running           0          28s\n",
      "alertmanager-main-1                    2/2     Running           0          28s\n",
      "alertmanager-main-2                    0/2     PodInitializing   0          28s\n",
      "blackbox-exporter-77966b4779-zbbrd     3/3     Running           0          42s\n",
      "grafana-5997747455-6xb96               1/1     Running           0          42s\n",
      "kube-state-metrics-756cd6cb9d-r6vnz    3/3     Running           0          41s\n",
      "node-exporter-bkhj4                    2/2     Running           0          41s\n",
      "node-exporter-bxp4r                    2/2     Running           0          41s\n",
      "node-exporter-jh4x5                    2/2     Running           0          41s\n",
      "prometheus-adapter-5794d7d9f5-cz8t6    1/1     Running           0          41s\n",
      "prometheus-adapter-5794d7d9f5-xfk8l    1/1     Running           0          41s\n",
      "prometheus-k8s-0                       0/2     PodInitializing   0          28s\n",
      "prometheus-k8s-1                       1/2     Running           0          28s\n",
      "prometheus-operator-56fd964fb9-57bck   2/2     Running           0          41s\n",
      "\n",
      "🎉 Suite de monitoring Prometheus et Grafana installée avec succès !\n"
     ]
    }
   ],
   "source": [
    "# Script pour déployer la suite de monitoring\n",
    "MONITORING_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- Installation de la suite Prometheus + Grafana via kube-prometheus ---\"\n",
    "\n",
    "# 1. Cloner le dépôt qui contient les manifestes\n",
    "if [ ! -d \"kube-prometheus\" ]; then\n",
    "    echo \"Clonage du dépôt kube-prometheus...\"\n",
    "    git clone https://github.com/prometheus-operator/kube-prometheus.git\n",
    "else\n",
    "    echo \"Dépôt kube-prometheus déjà présent.\"\n",
    "fi\n",
    "cd kube-prometheus\n",
    "\n",
    "# 2. Appliquer les définitions de ressources (CRDs) et la configuration de base\n",
    "echo \"Étape 1/3 : Application des CRDs et de la configuration 'setup'...\"\n",
    "kubectl apply --server-side -f manifests/setup\n",
    "\n",
    "# Attendre que les CRDs soient bien enregistrés dans le cluster avant de continuer\n",
    "echo \"Attente de l'établissement des CRDs...\"\n",
    "kubectl wait --for condition=Established --all CustomResourceDefinition --timeout=300s\n",
    "\n",
    "# 3. Appliquer le reste de la suite (Prometheus, Grafana, Alertmanager, etc.)\n",
    "echo \"Étape 2/3 : Application des manifestes de la suite de monitoring...\"\n",
    "kubectl apply -f manifests/\n",
    "\n",
    "# 4. Attendre que les déploiements clés soient prêts dans le namespace 'monitoring'\n",
    "echo \"Étape 3/3 : Attente du démarrage de Prometheus et Grafana (peut prendre quelques minutes)...\"\n",
    "kubectl wait --for=condition=available deployment/prometheus-k8s -n monitoring --timeout=300s\n",
    "kubectl wait --for=condition=available deployment/grafana -n monitoring --timeout=300s\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification des pods dans le namespace 'monitoring' ---\"\n",
    "kubectl get pods -n monitoring\n",
    "\n",
    "echo \"\\n🎉 Suite de monitoring Prometheus et Grafana installée avec succès !\"\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script sur le master\n",
    "print(\"Lancement de l'installation de la suite de monitoring...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(MONITORING_SCRIPT)\n",
    "\n",
    "# Affichage du résultat\n",
    "print(\"\\n--- Résultat de l'installation ---\")\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e8a1d",
   "metadata": {},
   "source": [
    "## Vérification des Composants de Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5812a18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686440daa8a34102bbf41e7c80ffc221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération de l'état des composants de monitoring...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pods dans le namespace 'monitoring' ---\n",
      "NAME                                   READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES\n",
      "alertmanager-main-0                    2/2     Running   0          2m36s   192.168.60.198   paradoxe-9   <none>           <none>\n",
      "alertmanager-main-1                    2/2     Running   0          2m36s   192.168.91.135   paradoxe-8   <none>           <none>\n",
      "alertmanager-main-2                    2/2     Running   0          2m36s   192.168.60.199   paradoxe-9   <none>           <none>\n",
      "blackbox-exporter-77966b4779-zbbrd     3/3     Running   0          2m50s   192.168.60.195   paradoxe-9   <none>           <none>\n",
      "grafana-5997747455-6xb96               1/1     Running   0          2m50s   192.168.60.196   paradoxe-9   <none>           <none>\n",
      "kube-state-metrics-756cd6cb9d-r6vnz    3/3     Running   0          2m49s   192.168.91.132   paradoxe-8   <none>           <none>\n",
      "node-exporter-bkhj4                    2/2     Running   0          2m49s   172.16.101.8     paradoxe-8   <none>           <none>\n",
      "node-exporter-bxp4r                    2/2     Running   0          2m49s   172.16.101.9     paradoxe-9   <none>           <none>\n",
      "node-exporter-jh4x5                    2/2     Running   0          2m49s   172.16.101.7     paradoxe-7   <none>           <none>\n",
      "prometheus-adapter-5794d7d9f5-cz8t6    1/1     Running   0          2m49s   192.168.60.197   paradoxe-9   <none>           <none>\n",
      "prometheus-adapter-5794d7d9f5-xfk8l    1/1     Running   0          2m49s   192.168.91.133   paradoxe-8   <none>           <none>\n",
      "prometheus-k8s-0                       2/2     Running   0          2m36s   192.168.60.200   paradoxe-9   <none>           <none>\n",
      "prometheus-k8s-1                       2/2     Running   0          2m36s   192.168.91.136   paradoxe-8   <none>           <none>\n",
      "prometheus-operator-56fd964fb9-57bck   2/2     Running   0          2m49s   192.168.91.134   paradoxe-8   <none>           <none>\n",
      "\n",
      "--- Services dans le namespace 'monitoring' ---\n",
      "NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\n",
      "alertmanager-main       ClusterIP   10.97.149.101    <none>        9093/TCP,8080/TCP            2m51s\n",
      "alertmanager-operated   ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP   2m36s\n",
      "blackbox-exporter       ClusterIP   10.108.30.66     <none>        9115/TCP,19115/TCP           2m50s\n",
      "grafana                 ClusterIP   10.108.195.102   <none>        3000/TCP                     2m50s\n",
      "kube-state-metrics      ClusterIP   None             <none>        8443/TCP,9443/TCP            2m49s\n",
      "node-exporter           ClusterIP   None             <none>        9100/TCP                     2m49s\n",
      "prometheus-adapter      ClusterIP   10.100.92.142    <none>        443/TCP                      2m49s\n",
      "prometheus-k8s          ClusterIP   10.96.8.133      <none>        9090/TCP,8080/TCP            2m49s\n",
      "prometheus-operated     ClusterIP   None             <none>        9090/TCP                     2m36s\n",
      "prometheus-operator     ClusterIP   None             <none>        8443/TCP                     2m49s\n",
      "\n",
      "--- ServiceAccounts dans le namespace 'monitoring' ---\n",
      "NAME                  SECRETS   AGE\n",
      "alertmanager-main     0         2m50s\n",
      "blackbox-exporter     0         2m50s\n",
      "default               0         2m59s\n",
      "grafana               0         2m50s\n",
      "kube-state-metrics    0         2m49s\n",
      "node-exporter         0         2m49s\n",
      "prometheus-adapter    0         2m49s\n",
      "prometheus-k8s        0         2m49s\n",
      "prometheus-operator   0         2m49s\n"
     ]
    }
   ],
   "source": [
    "# Script pour lister les pods et services de monitoring\n",
    "VERIFY_MONITORING_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"--- Pods dans le namespace 'monitoring' ---\"\n",
    "kubectl get pods -n monitoring -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Services dans le namespace 'monitoring' ---\"\n",
    "kubectl get services -n monitoring\n",
    "echo \"\"\n",
    "echo \"--- ServiceAccounts dans le namespace 'monitoring' ---\"\n",
    "kubectl get serviceaccounts -n monitoring\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script de vérification sur le master\n",
    "print(\"Récupération de l'état des composants de monitoring...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(VERIFY_MONITORING_SCRIPT)\n",
    "\n",
    "# Affichage du résultat\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab78fdd",
   "metadata": {},
   "source": [
    "## Mise à Jour des NetworkPolicies pour l'Accès Externe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34266c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44939a64b70a43e98d716969ece045e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour des NetworkPolicies...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Mise à jour de la NetworkPolicy de Grafana ---\n",
      "networkpolicy.networking.k8s.io/grafana unchanged\n",
      "\n",
      "--- 2. Mise à jour de la NetworkPolicy de Prometheus ---\n",
      "networkpolicy.networking.k8s.io/prometheus-k8s unchanged\n",
      "\n",
      "NetworkPolicies mises à jour avec succès pour autoriser l'accès externe.\n"
     ]
    }
   ],
   "source": [
    "# Script pour mettre à jour les NetworkPolicies\n",
    "UPDATE_NP_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- 1. Mise à jour de la NetworkPolicy de Grafana ---\"\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: grafana\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    app.kubernetes.io/component: grafana\n",
    "    app.kubernetes.io/name: grafana\n",
    "    app.kubernetes.io/part-of: kube-prometheus\n",
    "    app.kubernetes.io/version: 12.2.0\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/component: grafana\n",
    "      app.kubernetes.io/name: grafana\n",
    "      app.kubernetes.io/part-of: kube-prometheus\n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  - Egress\n",
    "  ingress:\n",
    "  - from:\n",
    "    # On autorise le trafic depuis n'importe quelle adresse IP (externe/interne)\n",
    "    - ipBlock:\n",
    "        cidr: 0.0.0.0/0\n",
    "    # On garde la règle existante qui autorise Prometheus\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app.kubernetes.io/name: prometheus\n",
    "    ports:\n",
    "    - port: 3000\n",
    "      protocol: TCP\n",
    "  egress:\n",
    "  - {}\n",
    "EOF\n",
    "\n",
    "echo \"\\n--- 2. Mise à jour de la NetworkPolicy de Prometheus ---\"\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: prometheus-k8s\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    app.kubernetes.io/component: prometheus\n",
    "    app.kubernetes.io/instance: k8s\n",
    "    app.kubernetes.io/name: prometheus\n",
    "    app.kubernetes.io/part-of: kube-prometheus\n",
    "    app.kubernetes.io/version: 3.6.0\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/component: prometheus\n",
    "      app.kubernetes.io/instance: k8s\n",
    "      app.kubernetes.io/name: prometheus\n",
    "      app.kubernetes.io/part-of: kube-prometheus\n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  - Egress\n",
    "  ingress:\n",
    "  - from:\n",
    "    # On autorise le trafic depuis n'importe quelle adresse IP\n",
    "    - ipBlock:\n",
    "        cidr: 0.0.0.0/0\n",
    "    # On garde les règles existantes pour la communication interne\n",
    "    - namespaceSelector: {}\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app.kubernetes.io/name: prometheus\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app.kubernetes.io/name: prometheus-adapter\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app.kubernetes.io/name: grafana\n",
    "    ports:\n",
    "    - port: 9090\n",
    "      protocol: TCP\n",
    "    - port: 8080\n",
    "      protocol: TCP\n",
    "  egress:\n",
    "  - {}\n",
    "EOF\n",
    "\n",
    "echo \"\\nNetworkPolicies mises à jour avec succès pour autoriser l'accès externe.\"\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script sur le master\n",
    "print(\"Mise à jour des NetworkPolicies...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(UPDATE_NP_SCRIPT)\n",
    "\n",
    "# Affichage du résultat\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306d0cb",
   "metadata": {},
   "source": [
    "## Passage des Services en NodePort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d4963c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1482a4d962143a28f537834cd66e7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch des services Grafana et Prometheus en NodePort...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Résultat de la commande patch ---\n",
      "--- 1. Modification du service Grafana en type NodePort ---\n",
      "service/grafana patched\n",
      "\n",
      "--- 2. Modification du service Prometheus en type NodePort ---\n",
      "service/prometheus-k8s patched\n",
      "\n",
      "--- 3. Vérification du résultat ---\n",
      "NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGE\n",
      "alertmanager-main       ClusterIP   10.97.149.101    <none>        9093/TCP,8080/TCP               8m33s\n",
      "alertmanager-operated   ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP      8m18s\n",
      "blackbox-exporter       ClusterIP   10.108.30.66     <none>        9115/TCP,19115/TCP              8m32s\n",
      "grafana                 NodePort    10.108.195.102   <none>        3000:32477/TCP                  8m32s\n",
      "kube-state-metrics      ClusterIP   None             <none>        8443/TCP,9443/TCP               8m31s\n",
      "node-exporter           ClusterIP   None             <none>        9100/TCP                        8m31s\n",
      "prometheus-adapter      ClusterIP   10.100.92.142    <none>        443/TCP                         8m31s\n",
      "prometheus-k8s          NodePort    10.96.8.133      <none>        9090:31352/TCP,8080:32246/TCP   8m31s\n",
      "prometheus-operated     ClusterIP   None             <none>        9090/TCP                        8m18s\n",
      "prometheus-operator     ClusterIP   None             <none>        8443/TCP                        8m31s\n"
     ]
    }
   ],
   "source": [
    "# Script pour patcher les services en NodePort\n",
    "PATCH_SERVICES_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- 1. Modification du service Grafana en type NodePort ---\"\n",
    "kubectl patch service grafana -n monitoring -p '{\"spec\": {\"type\": \"NodePort\"}}'\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 2. Modification du service Prometheus en type NodePort ---\"\n",
    "kubectl patch service prometheus-k8s -n monitoring -p '{\"spec\": {\"type\": \"NodePort\"}}'\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 3. Vérification du résultat ---\"\n",
    "# Petite pause pour s'assurer que l'API server a bien traité les changements\n",
    "sleep 2\n",
    "kubectl get services -n monitoring\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script sur le master\n",
    "print(\"Patch des services Grafana et Prometheus en NodePort...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(PATCH_SERVICES_SCRIPT)\n",
    "\n",
    "# Affichage du résultat\n",
    "print(\"\\n--- Résultat de la commande patch ---\")\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d17f3",
   "metadata": {},
   "source": [
    "#  Installation de Kepler pour le Monitoring Énergétique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dadaf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a85d2a1ff634c5da073f6528971ab4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le script d'installation final a été sauvegardé dans 'install_kepler.sh'.\n",
      "Lancement de la mise à jour de Kepler...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">script</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mscript\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helm non trouvé. Installation de Helm...\n",
      "Downloading https://get.helm.sh/helm-v3.19.0-linux-amd64.tar.gz\n",
      "Verifying checksum... Done.\n",
      "Preparing to install helm into /usr/local/bin\n",
      "helm installed into /usr/local/bin/helm\n",
      "\n",
      "--- Ajout et mise à jour du dépôt Helm de Kepler ---\n",
      "\"kepler\" has been added to your repositories\n",
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"kepler\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n",
      "\n",
      "--- Déploiement/Mise à jour de Kepler dans le namespace 'kepler' ---\n",
      "Release \"kepler\" does not exist. Installing it now.\n",
      "NAME: kepler\n",
      "LAST DEPLOYED: Wed Oct  8 13:16:50 2025\n",
      "NAMESPACE: kepler\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "\n",
      "--- Attente des composants Kepler ---\n",
      "pod/kepler-6hnl6 condition met\n",
      "pod/kepler-j5w5h condition met\n",
      "pod/kepler-jzgl9 condition met\n",
      "\n",
      "--- Vérification des pods Kepler ---\n",
      "NAME           READY   STATUS    RESTARTS   AGE\n",
      "kepler-6hnl6   1/1     Running   0          21s\n",
      "kepler-j5w5h   1/1     Running   0          21s\n",
      "kepler-jzgl9   1/1     Running   0          21s\n",
      "\n",
      "--- Vérification que le ServiceMonitor a bien été créé ---\n",
      "NAME                         AGE\n",
      "kepler-prometheus-exporter   21s\n",
      "--- Vérification que le service est bien de type NodePort ---\n",
      "NAME     TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\n",
      "kepler   NodePort   10.101.151.180   <none>        9102:32340/TCP   21s\n",
      "\n",
      "🎉 Kepler est maintenant correctement configuré avec un ServiceMonitor et un NodePort !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Script mis à jour avec le bon paramètre pour le ServiceMonitor\n",
    "INSTALL_KEPLER_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "# --- 1. Installation de Helm (si non présent) ---\n",
    "if ! command -v helm &> /dev/null\n",
    "then\n",
    "    echo \"Helm non trouvé. Installation de Helm...\"\n",
    "    curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n",
    "    chmod 700 get_helm.sh\n",
    "    ./get_helm.sh\n",
    "else\n",
    "    echo \"Helm est déjà installé.\"\n",
    "fi\n",
    "\n",
    "# --- 2. Ajout du dépôt Helm de Kepler ---\n",
    "echo \"\"\n",
    "echo \"--- Ajout et mise à jour du dépôt Helm de Kepler ---\"\n",
    "helm repo add kepler https://sustainable-computing-io.github.io/kepler-helm-chart\n",
    "helm repo update\n",
    "\n",
    "# --- 3. Installation ou Mise à Jour de Kepler ---\n",
    "echo \"\"\n",
    "echo \"--- Déploiement/Mise à jour de Kepler dans le namespace 'kepler' ---\"\n",
    "# CORRECTION : On utilise 'serviceMonitor.enabled=true'\n",
    "helm upgrade --install kepler kepler/kepler --namespace kepler --create-namespace \\\n",
    "    --set serviceMonitor.enabled=true \\\n",
    "    --set service.type=NodePort\n",
    "\n",
    "# --- 4. Vérification de l'installation ---\n",
    "echo \"\"\n",
    "echo \"--- Attente des composants Kepler ---\"\n",
    "kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=kepler -n kepler --timeout=300s\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification des pods Kepler ---\"\n",
    "kubectl get pods -n kepler\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification que le ServiceMonitor a bien été créé ---\"\n",
    "kubectl get servicemonitor -n kepler\n",
    "\n",
    "echo \"--- Vérification que le service est bien de type NodePort ---\"\n",
    "kubectl get service -n kepler\n",
    "\n",
    "echo \"\"\n",
    "echo \"🎉 Kepler est maintenant correctement configuré avec un ServiceMonitor et un NodePort !\"\n",
    "\"\"\"\n",
    "\n",
    "# --- Étape 1 : Enregistrer le script dans un fichier local ---\n",
    "script_path = \"install_kepler.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(INSTALL_KEPLER_SCRIPT)\n",
    "os.chmod(script_path, 0o755)\n",
    "print(f\"Le script d'installation final a été sauvegardé dans '{script_path}'.\")\n",
    "\n",
    "\n",
    "# --- Étape 2 : Exécuter le fichier script avec enoslib ---\n",
    "print(\"Lancement de la mise à jour de Kepler...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.script(script_path)\n",
    "\n",
    "# Affichage du résultat\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e625f",
   "metadata": {},
   "source": [
    "## Vérification des ServiceMonitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "247cec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be45561375ae44ce9da31e4cb54bff88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for ServiceMonitor resources across the cluster...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Recherche des ServiceMonitors dans tous les namespaces (-A) ---\n",
      "NAMESPACE    NAME                         AGE\n",
      "kepler       kepler-prometheus-exporter   2m25s\n",
      "monitoring   alertmanager-main            11m\n",
      "monitoring   blackbox-exporter            11m\n",
      "monitoring   coredns                      11m\n",
      "monitoring   grafana                      11m\n",
      "monitoring   kube-apiserver               11m\n",
      "monitoring   kube-controller-manager      11m\n",
      "monitoring   kube-scheduler               11m\n",
      "monitoring   kube-state-metrics           11m\n",
      "monitoring   kubelet                      11m\n",
      "monitoring   node-exporter                11m\n",
      "monitoring   prometheus-adapter           11m\n",
      "monitoring   prometheus-k8s               11m\n",
      "monitoring   prometheus-operator          11m\n"
     ]
    }
   ],
   "source": [
    "# Script to list ServiceMonitors\n",
    "CHECK_SM_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"--- Recherche des ServiceMonitors dans tous les namespaces (-A) ---\"\n",
    "kubectl get servicemonitor -A\n",
    "\"\"\"\n",
    "\n",
    "# Execute the verification script on the master node\n",
    "print(\"Checking for ServiceMonitor resources across the cluster...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(CHECK_SM_SCRIPT)\n",
    "\n",
    "# Display the result\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1d2e5",
   "metadata": {},
   "source": [
    "## Suppression Complète de Kepler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8e17cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52b46ba139647188b8206c2002196a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de la suppression complète de Kepler...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Désinstallation de la release Helm 'kepler' ---\n",
      "NAME: kepler\n",
      "LAST DEPLOYED: Wed Oct  8 13:16:50 2025\n",
      "NAMESPACE: kepler\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "release \"kepler\" uninstalled\n",
      "\n",
      "--- 2. Suppression du ServiceMonitor dans le namespace 'monitoring' ---\n",
      "\n",
      "--- 3. Suppression du namespace 'kepler' ---\n",
      "namespace \"kepler\" deleted\n",
      "\n",
      "--- 4. Suppression du dépôt Helm de la configuration locale ---\n",
      "\"kepler\" has been removed from your repositories\n",
      "\n",
      "🎉 Nettoyage de Kepler terminé.\n"
     ]
    }
   ],
   "source": [
    "# Script pour supprimer complètement Kepler\n",
    "UNINSTALL_KEPLER_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"--- 1. Désinstallation de la release Helm 'kepler' ---\"\n",
    "# On vérifie si la release existe avant de tenter de la supprimer\n",
    "if helm status kepler -n kepler &> /dev/null; then\n",
    "    helm uninstall kepler -n kepler\n",
    "else\n",
    "    echo \"Release Helm 'kepler' non trouvée.\"\n",
    "fi\n",
    "\n",
    "echo \"\\n--- 2. Suppression du ServiceMonitor dans le namespace 'monitoring' ---\"\n",
    "# --ignore-not-found=true évite une erreur si la ressource a déjà été supprimée\n",
    "kubectl delete servicemonitor kepler-prometheus-exporter -n monitoring --ignore-not-found=true\n",
    "\n",
    "echo \"\\n--- 3. Suppression du namespace 'kepler' ---\"\n",
    "kubectl delete namespace kepler --ignore-not-found=true\n",
    "\n",
    "echo \"\\n--- 4. Suppression du dépôt Helm de la configuration locale ---\"\n",
    "if helm repo list | grep -q \"kepler\"; then\n",
    "    helm repo remove kepler\n",
    "else\n",
    "    echo \"Dépôt Helm 'kepler' non trouvé.\"\n",
    "fi\n",
    "\n",
    "echo \"\\n🎉 Nettoyage de Kepler terminé.\"\n",
    "\"\"\"\n",
    "\n",
    "# Exécution du script de suppression sur le master\n",
    "print(\"Lancement de la suppression complète de Kepler...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(UNINSTALL_KEPLER_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ef0ab",
   "metadata": {},
   "source": [
    "# Installer Kepler depuis les Manifestes Locaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7236bc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a0051c674445058b0da793b6c47173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Copie du répertoire local 'kepler' vers '/tmp/kepler' sur le master... ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">copy</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mcopy\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2a7ca0913c43faba5aa28d9a9fa7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire copié avec succès.\n",
      "\n",
      "--- Application des manifestes sur le cluster... ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Application de tous les manifestes depuis le répertoire /tmp/kepler ---\n",
      "configmap/kepler created\n",
      "daemonset.apps/kepler created\n",
      "namespace/kepler unchanged\n",
      "role.rbac.authorization.k8s.io/prom-kepler unchanged\n",
      "rolebinding.rbac.authorization.k8s.io/prom-kepler unchanged\n",
      "serviceaccount/kepler unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/kepler unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/kepler unchanged\n",
      "service/kepler unchanged\n",
      "servicemonitor.monitoring.coreos.com/kepler unchanged\n",
      "\n",
      "--- Attente de 30 secondes pour le démarrage des pods ---\n",
      "\n",
      "--- Vérification du statut des pods dans le namespace 'kepler' ---\n",
      "NAME           READY   STATUS    RESTARTS   AGE\n",
      "kepler-bk55j   1/1     Running   0          31s\n",
      "kepler-g7vv6   1/1     Running   0          31s\n",
      "kepler-khdrz   1/1     Running   0          31s\n",
      "--- Vérification que le Service a bien été créé ---\n",
      "NAME     TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE    SELECTOR\n",
      "kepler   NodePort   10.108.116.210   <none>        28282:30440/TCP   8m3s   app.kubernetes.io/name=kepler,app.kubernetes.io/part-of=kepler\n",
      "\n",
      "🎉 Kepler a été déployé à partir des manifestes locaux.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- Étape 1 : Copier le répertoire local 'kepler' vers le master ---\n",
    "\n",
    "source_dir = \"kepler\"\n",
    "dest_dir = \"/tmp/kepler\"\n",
    "\n",
    "if not os.path.isdir(source_dir):\n",
    "    raise FileNotFoundError(f\"Le répertoire local '{source_dir}' est introuvable.\")\n",
    "\n",
    "print(f\"--- Copie du répertoire local '{source_dir}' vers '{dest_dir}' sur le master... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=source_dir, dest=\"/tmp/\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"Répertoire copié avec succès.\")\n",
    "else:\n",
    "    print(\"Erreur lors de la copie du répertoire.\")\n",
    "    if p.results:\n",
    "        print(p.results[0].stderr)\n",
    "\n",
    "\n",
    "# --- Étape 2 : Appliquer les manifestes et vérifier ---\n",
    "\n",
    "# Script simplifié : la création du namespace est gérée par les manifestes\n",
    "APPLY_KEPLER_MANIFESTS_SCRIPT = f\"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- Application de tous les manifestes depuis le répertoire {dest_dir} ---\"\n",
    "kubectl apply -f \"{dest_dir}\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente de 30 secondes pour le démarrage des pods ---\"\n",
    "sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification du statut des pods dans le namespace 'kepler' ---\"\n",
    "kubectl get pods -n kepler\n",
    "\n",
    "echo \"--- Vérification que le Service a bien été créé ---\"\n",
    "kubectl get svc -n kepler -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"🎉 Kepler a été déployé à partir des manifestes locaux.\"\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n--- Application des manifestes sur le cluster... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(APPLY_KEPLER_MANIFESTS_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b551dc",
   "metadata": {},
   "source": [
    "## Diagnostic - Pourquoi Prometheus ne Scrape pas Kepler ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9602aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a3a73298264035af7d35e4e629cc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Lancement du diagnostic complet Prometheus ↔ Kepler...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">script</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-13.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mscript\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-13.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🔍 DIAGNOSTIC COMPLET: Prometheus ↔ Kepler\n",
      "======================================================================\n",
      "\n",
      "--- 1. Vérification des Pods Kepler ---\n",
      "NAME           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES\n",
      "kepler-6hv64   1/1     Running   0          47m   192.168.64.10   paradoxe-26   <none>           <none>\n",
      "kepler-8nqbt   1/1     Running   0          47m   192.168.0.196   paradoxe-13   <none>           <none>\n",
      "kepler-tdxsq   1/1     Running   0          47m   192.168.64.72   paradoxe-19   <none>           <none>\n",
      "\n",
      "--- 2. Vérification du Service Kepler ---\n",
      "NAME     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE   SELECTOR\n",
      "kepler   NodePort   10.111.96.169   <none>        28282:31515/TCP   48m   app.kubernetes.io/name=kepler,app.kubernetes.io/part-of=kepler\n",
      "\n",
      "--- 3. Vérification des Labels du Service Kepler ---\n",
      "Les labels sont CRITIQUES pour que le ServiceMonitor fonctionne!\n",
      "    labels:\n",
      "      app.kubernetes.io/name: kepler\n",
      "      app.kubernetes.io/part-of: kepler\n",
      "    name: kepler\n",
      "    namespace: kepler\n",
      "    resourceVersion: \"5141\"\n",
      "    uid: 5a752599-4a3f-4d3c-b13f-658671e9b47b\n",
      "  spec:\n",
      "    clusterIP: 10.111.96.169\n",
      "    clusterIPs:\n",
      "    - 10.111.96.169\n",
      "\n",
      "--- 4. Vérification du ServiceMonitor Kepler ---\n",
      "NAME     AGE\n",
      "kepler   48m\n",
      "\n",
      "--- 5. Détails du ServiceMonitor (selector et labels) ---\n",
      "    selector:\n",
      "      matchLabels:\n",
      "        app.kubernetes.io/name: kepler\n",
      "        app.kubernetes.io/part-of: kepler\n",
      "kind: List\n",
      "metadata:\n",
      "  resourceVersion: \"\"\n",
      "\n",
      "--- 6. Vérification que Prometheus-Operator voit le ServiceMonitor ---\n",
      "Prometheus doit être dans le même namespace ou avec les bonnes règles RBAC\n",
      "NAMESPACE    NAME                      AGE\n",
      "kepler       kepler                    48m\n",
      "monitoring   alertmanager-main         78m\n",
      "monitoring   blackbox-exporter         78m\n",
      "monitoring   coredns                   78m\n",
      "monitoring   grafana                   78m\n",
      "monitoring   kube-apiserver            78m\n",
      "monitoring   kube-controller-manager   78m\n",
      "monitoring   kube-scheduler            78m\n",
      "monitoring   kube-state-metrics        78m\n",
      "monitoring   kubelet                   78m\n",
      "monitoring   node-exporter             78m\n",
      "monitoring   prometheus-adapter        78m\n",
      "monitoring   prometheus-k8s            78m\n",
      "monitoring   prometheus-operator       78m\n",
      "\n",
      "--- 7. Configuration de Prometheus pour ServiceMonitor ---\n",
      "Vérification des serviceMonitorSelector dans Prometheus:\n",
      "    serviceMonitorSelector: {}\n",
      "    version: 3.6.0\n",
      "  status:\n",
      "    availableReplicas: 2\n",
      "    conditions:\n",
      "    - lastTransitionTime: \"2025-10-08T07:38:42Z\"\n",
      "\n",
      "--- 8. Vérification des Endpoints Kepler ---\n",
      "Si pas d'endpoints, le service ne pointe vers rien!\n",
      "NAME     ENDPOINTS                                                     AGE\n",
      "kepler   192.168.0.196:28282,192.168.64.10:28282,192.168.64.72:28282   48m\n",
      "\n",
      "--- 9. Test direct du endpoint Kepler ---\n",
      "Test de l'endpoint metrics sur le pod kepler-6hv64:\n",
      "command terminated with exit code 7\n",
      "\n",
      "--- 10. Vérification des Logs Prometheus ---\n",
      "Recherche d'erreurs liées à Kepler dans les logs Prometheus:\n",
      "time=2025-10-08T08:09:45.475Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:09:45.476Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:10:19.375Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:10:19.375Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:11:02.556Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:11:02.556Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:11:44.904Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:11:44.905Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:12:28.690Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:12:28.690Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:13:16.938Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:13:16.938Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:14:07.792Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:14:07.792Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:14:43.975Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:14:43.975Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:15:22.511Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:15:22.511Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:15:57.075Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:15:57.076Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:16:54.319Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:16:54.319Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:17:32.825Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:17:32.825Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:18:21.062Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:18:21.062Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:18:52.692Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:18:52.692Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:19:40.358Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:19:40.358Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:20:38.488Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:20:38.488Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:21:29.916Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:21:29.916Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:22:07.575Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:22:07.576Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:22:45.036Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:22:45.036Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:23:44.884Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:23:44.885Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:24:16.270Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:24:16.270Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:24:48.259Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:24:48.259Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:25:25.859Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:25:25.859Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:26:05.775Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:26:05.775Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "time=2025-10-08T08:27:00.009Z level=INFO source=reflector.go:569 msg=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\" component=k8s_client_runtime\n",
      "time=2025-10-08T08:27:00.010Z level=ERROR source=reflector.go:166 msg=\"Unhandled Error\" component=k8s_client_runtime logger=UnhandledError err=\"pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User \\\"system:serviceaccount:monitoring:prometheus-k8s\\\" cannot list resource \\\"endpointslices\\\" in API group \\\"discovery.k8s.io\\\" in the namespace \\\"kepler\\\"\"\n",
      "\n",
      "--- 11. Configuration Actuelle des Targets Prometheus ---\n",
      "Vérifiez si Kepler apparaît dans les targets de Prometheus\n",
      "💡 Accédez à l'UI Prometheus: Status → Targets\n",
      "💡 Ou utilisez l'API: curl http://prometheus-ip:port/api/v1/targets\n",
      "\n",
      "======================================================================\n",
      "🎯 CAUSES COMMUNES ET SOLUTIONS:\n",
      "======================================================================\n",
      "\n",
      "❌ PROBLÈME 1: Labels incompatibles\n",
      "   Le ServiceMonitor cherche des labels spécifiques sur le Service\n",
      "   Solution: Vérifiez que les labels du Service matchent le selector du ServiceMonitor\n",
      "\n",
      "❌ PROBLÈME 2: Namespace incorrect\n",
      "   Le ServiceMonitor doit être dans un namespace que Prometheus surveille\n",
      "   Solution: Soit mettre le ServiceMonitor dans 'monitoring', soit configurer Prometheus\n",
      "\n",
      "❌ PROBLÈME 3: serviceMonitorSelector vide\n",
      "   Prometheus n'est configuré pour surveiller AUCUN ServiceMonitor\n",
      "   Solution: Configurer prometheus.serviceMonitorSelector: {}\n",
      "\n",
      "❌ PROBLÈME 4: Port incorrect\n",
      "   Le ServiceMonitor pointe vers un port qui n'existe pas\n",
      "   Solution: Vérifier que le port dans ServiceMonitor = port du Service\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Script de diagnostic complet pour identifier pourquoi Prometheus ne scrape pas Kepler\n",
    "DIAGNOSTIC_KEPLER_PROMETHEUS = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"======================================================================\"\n",
    "echo \"🔍 DIAGNOSTIC COMPLET: Prometheus ↔ Kepler\"\n",
    "echo \"======================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 1. Vérification des Pods Kepler ---\"\n",
    "kubectl get pods -n kepler -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 2. Vérification du Service Kepler ---\"\n",
    "kubectl get svc -n kepler -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 3. Vérification des Labels du Service Kepler ---\"\n",
    "echo \"Les labels sont CRITIQUES pour que le ServiceMonitor fonctionne!\"\n",
    "kubectl get svc -n kepler -o yaml | grep -A 10 \"labels:\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 4. Vérification du ServiceMonitor Kepler ---\"\n",
    "kubectl get servicemonitor -n kepler -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 5. Détails du ServiceMonitor (selector et labels) ---\"\n",
    "kubectl get servicemonitor -n kepler -o yaml | grep -A 20 \"selector:\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 6. Vérification que Prometheus-Operator voit le ServiceMonitor ---\"\n",
    "echo \"Prometheus doit être dans le même namespace ou avec les bonnes règles RBAC\"\n",
    "kubectl get servicemonitor -A\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 7. Configuration de Prometheus pour ServiceMonitor ---\"\n",
    "echo \"Vérification des serviceMonitorSelector dans Prometheus:\"\n",
    "kubectl get prometheus -n monitoring -o yaml | grep -A 5 \"serviceMonitorSelector:\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 8. Vérification des Endpoints Kepler ---\"\n",
    "echo \"Si pas d'endpoints, le service ne pointe vers rien!\"\n",
    "kubectl get endpoints -n kepler\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 9. Test direct du endpoint Kepler ---\"\n",
    "KEPLER_POD=`kubectl get pods -n kepler -l app.kubernetes.io/name=kepler -o jsonpath='{.items[0].metadata.name}'`\n",
    "if [ -n \"$KEPLER_POD\" ]; then\n",
    "    echo \"Test de l'endpoint metrics sur le pod $KEPLER_POD:\"\n",
    "    kubectl exec -n kepler $KEPLER_POD -- curl -s localhost:9102/metrics | head -20\n",
    "else\n",
    "    echo \"❌ Aucun pod Kepler trouvé!\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 10. Vérification des Logs Prometheus ---\"\n",
    "echo \"Recherche d'erreurs liées à Kepler dans les logs Prometheus:\"\n",
    "PROM_POD=`kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}'`\n",
    "if [ -n \"$PROM_POD\" ]; then\n",
    "    kubectl logs -n monitoring $PROM_POD -c prometheus --tail=50 | grep -i kepler || echo \"Aucune mention de Kepler dans les logs récents\"\n",
    "else\n",
    "    echo \"❌ Pod Prometheus non trouvé!\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 11. Configuration Actuelle des Targets Prometheus ---\"\n",
    "echo \"Vérifiez si Kepler apparaît dans les targets de Prometheus\"\n",
    "echo \"💡 Accédez à l'UI Prometheus: Status → Targets\"\n",
    "echo \"💡 Ou utilisez l'API: curl http://prometheus-ip:port/api/v1/targets\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"======================================================================\"\n",
    "echo \"🎯 CAUSES COMMUNES ET SOLUTIONS:\"\n",
    "echo \"======================================================================\"\n",
    "echo \"\"\n",
    "echo \"❌ PROBLÈME 1: Labels incompatibles\"\n",
    "echo \"   Le ServiceMonitor cherche des labels spécifiques sur le Service\"\n",
    "echo \"   Solution: Vérifiez que les labels du Service matchent le selector du ServiceMonitor\"\n",
    "echo \"\"\n",
    "echo \"❌ PROBLÈME 2: Namespace incorrect\"\n",
    "echo \"   Le ServiceMonitor doit être dans un namespace que Prometheus surveille\"\n",
    "echo \"   Solution: Soit mettre le ServiceMonitor dans 'monitoring', soit configurer Prometheus\"\n",
    "echo \"\"\n",
    "echo \"❌ PROBLÈME 3: serviceMonitorSelector vide\"\n",
    "echo \"   Prometheus n'est configuré pour surveiller AUCUN ServiceMonitor\"\n",
    "echo \"   Solution: Configurer prometheus.serviceMonitorSelector: {}\"\n",
    "echo \"\"\n",
    "echo \"❌ PROBLÈME 4: Port incorrect\"\n",
    "echo \"   Le ServiceMonitor pointe vers un port qui n'existe pas\"\n",
    "echo \"   Solution: Vérifier que le port dans ServiceMonitor = port du Service\"\n",
    "echo \"\"\n",
    "echo \"======================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "# --- Enregistrer le script dans un fichier local ---\n",
    "script_path = \"diagnostic_kepler.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(DIAGNOSTIC_KEPLER_PROMETHEUS)\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(\"🔍 Lancement du diagnostic complet Prometheus ↔ Kepler...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.script(script_path)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c1d10",
   "metadata": {},
   "source": [
    "## 🔧 Solution : Donner les Permissions RBAC à Prometheus pour Kepler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d3187c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e1b08165644240a689fd74e36a0c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Application de la correction RBAC...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "🔧 Correction des Permissions RBAC pour Prometheus → Kepler\n",
      "========================================================================\n",
      "\n",
      "--- Création d'un ClusterRole avec les permissions nécessaires ---\n",
      "clusterrole.rbac.authorization.k8s.io/prometheus-kepler-access created\n",
      "\n",
      "--- Liaison du ClusterRole au ServiceAccount prometheus-k8s ---\n",
      "clusterrolebinding.rbac.authorization.k8s.io/prometheus-kepler-access created\n",
      "\n",
      "--- Vérification des permissions créées ---\n",
      "NAME                       CREATED AT\n",
      "prometheus-kepler-access   2025-10-08T11:25:48Z\n",
      "NAME                       ROLE                                   AGE\n",
      "prometheus-kepler-access   ClusterRole/prometheus-kepler-access   1s\n",
      "\n",
      "--- Redémarrage des pods Prometheus pour appliquer les changements ---\n",
      "statefulset.apps/prometheus-k8s restarted\n",
      "\n",
      "--- Attente du redémarrage de Prometheus (30 secondes) ---\n",
      "\n",
      "--- Vérification que Prometheus est prêt ---\n",
      "pod/prometheus-k8s-0 condition met\n",
      "pod/prometheus-k8s-1 condition met\n",
      "\n",
      "========================================================================\n",
      "✅ Permissions RBAC configurées avec succès !\n",
      "========================================================================\n",
      "\n",
      "Prometheus peut maintenant accéder aux EndpointSlices du namespace kepler.\n",
      "Attendez 1-2 minutes puis vérifiez les targets dans l'UI Prometheus.\n"
     ]
    }
   ],
   "source": [
    "# Script pour corriger les permissions RBAC de Prometheus\n",
    "FIX_PROMETHEUS_RBAC = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"🔧 Correction des Permissions RBAC pour Prometheus → Kepler\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Création d'un ClusterRole avec les permissions nécessaires ---\"\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRole\n",
    "metadata:\n",
    "  name: prometheus-kepler-access\n",
    "rules:\n",
    "- apiGroups: [\"\"]\n",
    "  resources:\n",
    "  - services\n",
    "  - endpoints\n",
    "  - pods\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "- apiGroups: [\"discovery.k8s.io\"]\n",
    "  resources:\n",
    "  - endpointslices\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Liaison du ClusterRole au ServiceAccount prometheus-k8s ---\"\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: prometheus-kepler-access\n",
    "roleRef:\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "  kind: ClusterRole\n",
    "  name: prometheus-kepler-access\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: prometheus-k8s\n",
    "  namespace: monitoring\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification des permissions créées ---\"\n",
    "kubectl get clusterrole prometheus-kepler-access\n",
    "kubectl get clusterrolebinding prometheus-kepler-access\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Redémarrage des pods Prometheus pour appliquer les changements ---\"\n",
    "kubectl rollout restart statefulset prometheus-k8s -n monitoring\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente du redémarrage de Prometheus (30 secondes) ---\"\n",
    "sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification que Prometheus est prêt ---\"\n",
    "kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus -n monitoring --timeout=120s\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"✅ Permissions RBAC configurées avec succès !\"\n",
    "echo \"========================================================================\"\n",
    "echo \"\"\n",
    "echo \"Prometheus peut maintenant accéder aux EndpointSlices du namespace kepler.\"\n",
    "echo \"Attendez 1-2 minutes puis vérifiez les targets dans l'UI Prometheus.\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔧 Application de la correction RBAC...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(FIX_PROMETHEUS_RBAC)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c02f4",
   "metadata": {},
   "source": [
    "# Déploiement des Ressources PowerCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6c5d05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4401781c0c4fff8c66e821782c7309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Copie du répertoire 'powercap' vers '/tmp/powercap' sur le master... ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">copy</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mcopy\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6222535af447b18f24e53f2d3e088f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Répertoire copié avec succès.\n",
      "\n",
      "--- Déploiement des ressources PowerCap... ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "🚀 Déploiement des Ressources PowerCap dans le namespace 'default'\n",
      "========================================================================\n",
      "\n",
      "--- Application de tous les manifestes depuis /tmp/powercap ---\n",
      "configmap/powercap-config unchanged\n",
      "daemonset.apps/powercap-manager configured\n",
      "serviceaccount/powercap-manager unchanged\n",
      "clusterrole.rbac.authorization.k8s.io/powercap-manager unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io/powercap-manager unchanged\n",
      "\n",
      "--- Vérification du DaemonSet powercap-manager ---\n",
      "NAME               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS         IMAGES                                SELECTOR\n",
      "powercap-manager   2         2         2       2            2           <none>          40m   powercap-manager   ghcr.io/menraromial/powercap:latest   app=powercap-manager\n",
      "\n",
      "--- Vérification des pods powercap-manager ---\n",
      "NAME                     READY   STATUS    RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES\n",
      "powercap-manager-jb72g   1/1     Running   0          15m   172.16.101.9   paradoxe-9   <none>           <none>\n",
      "powercap-manager-tjpl4   1/1     Running   0          14m   172.16.101.8   paradoxe-8   <none>           <none>\n",
      "\n",
      "--- Statut du DaemonSet (nombre de pods prêts) ---\n",
      "daemon set \"powercap-manager\" successfully rolled out\n",
      "\n",
      "--- Vérification du ServiceAccount ---\n",
      "NAME               SECRETS   AGE\n",
      "powercap-manager   0         40m\n",
      "\n",
      "--- Vérification de la ConfigMap ---\n",
      "NAME              DATA   AGE\n",
      "powercap-config   8      40m\n",
      "\n",
      "--- Vérification des services (si présents) ---\n",
      "\n",
      "========================================================================\n",
      "✅ Déploiement PowerCap terminé !\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- Script pour déployer tous les manifestes PowerCap ---\n",
    "\n",
    "source_dir = \"powercap\"\n",
    "dest_dir = \"/tmp/powercap\"\n",
    "\n",
    "# Vérifier que le répertoire local existe\n",
    "if not os.path.isdir(source_dir):\n",
    "    print(f\"⚠️  Le répertoire local '{source_dir}' n'existe pas.\")\n",
    "    print(f\"📁 Veuillez créer le dossier '{source_dir}' avec les fichiers suivants:\")\n",
    "    print(\"   - daemonset.yaml\")\n",
    "    print(\"   - rbac.yaml\")\n",
    "    print(\"   - configmap.yaml\")\n",
    "    print(\"\\n💡 Ou uploadez le dossier complet dans le répertoire de travail.\")\n",
    "    raise FileNotFoundError(f\"Le répertoire local '{source_dir}' est introuvable. Créez-le d'abord avec les manifestes nécessaires.\")\n",
    "\n",
    "# --- Étape 1 : Copier le répertoire local vers le master ---\n",
    "print(f\"--- Copie du répertoire '{source_dir}' vers '{dest_dir}' sur le master... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=source_dir, dest=\"/tmp/\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"✅ Répertoire copié avec succès.\")\n",
    "else:\n",
    "    print(\"❌ Erreur lors de la copie du répertoire.\")\n",
    "    if p.results:\n",
    "        print(p.results[0].stderr)\n",
    "\n",
    "# --- Étape 2 : Appliquer tous les manifestes ---\n",
    "DEPLOY_POWERCAP_SCRIPT = f\"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"🚀 Déploiement des Ressources PowerCap dans le namespace 'default'\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Application de tous les manifestes depuis {dest_dir} ---\"\n",
    "kubectl apply -f \"{dest_dir}\"\n",
    "\n",
    "# echo \"\"\n",
    "# echo \"--- Attente de 30 secondes pour la création des ressources ---\"\n",
    "# sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification du DaemonSet powercap-manager ---\"\n",
    "kubectl get daemonset powercap-manager -n default -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification des pods powercap-manager ---\"\n",
    "kubectl get pods -n default -l app=powercap-manager -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Statut du DaemonSet (nombre de pods prêts) ---\"\n",
    "kubectl rollout status daemonset/powercap-manager -n default --timeout=60s\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification du ServiceAccount ---\"\n",
    "kubectl get serviceaccount powercap-manager -n default\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification de la ConfigMap ---\"\n",
    "kubectl get configmap powercap-config -n default\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification des services (si présents) ---\"\n",
    "kubectl get services -n default -l app=powercap-manager || echo \"Aucun service trouvé\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"✅ Déploiement PowerCap terminé !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Déploiement des ressources PowerCap... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DEPLOY_POWERCAP_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d700f",
   "metadata": {},
   "source": [
    "## Vérification de l'État et des Logs PowerCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19d50d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165695167f2346389ab8b0a0c7d5bed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Lancement de la vérification détaillée de l'état et des logs PowerCap...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "📊 VÉRIFICATION DÉTAILLÉE - État et Logs PowerCap Manager\n",
      "========================================================================\n",
      "\n",
      "--- 1. État du DaemonSet powercap-manager ---\n",
      "NAME               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS         IMAGES                                SELECTOR\n",
      "powercap-manager   2         2         2       2            2           <none>          28m   powercap-manager   ghcr.io/menraromial/powercap:latest   app=powercap-manager\n",
      "\n",
      "--- 2. Détails du DaemonSet ---\n",
      "Name:           powercap-manager\n",
      "Selector:       app=powercap-manager\n",
      "Node-Selector:  <none>\n",
      "Labels:         app=powercap-manager\n",
      "                app.kubernetes.io/component=power-management\n",
      "                app.kubernetes.io/name=powercap-manager\n",
      "                app.kubernetes.io/version=v1.0.0\n",
      "Annotations:    deprecated.daemonset.template.generation: 2\n",
      "Desired Number of Nodes Scheduled: 2\n",
      "Current Number of Nodes Scheduled: 2\n",
      "Number of Nodes Scheduled with Up-to-date Pods: 2\n",
      "Number of Nodes Scheduled with Available Pods: 2\n",
      "Number of Nodes Misscheduled: 0\n",
      "Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed\n",
      "Pod Template:\n",
      "  Labels:           app=powercap-manager\n",
      "                    app.kubernetes.io/component=power-management\n",
      "                    app.kubernetes.io/name=powercap-manager\n",
      "                    app.kubernetes.io/version=v1.0.0\n",
      "  Service Account:  powercap-manager\n",
      "  Containers:\n",
      "   powercap-manager:\n",
      "    Image:      ghcr.io/menraromial/powercap:latest\n",
      "    Port:       <none>\n",
      "    Host Port:  <none>\n",
      "    Limits:\n",
      "      cpu:     200m\n",
      "      memory:  128Mi\n",
      "    Requests:\n",
      "      cpu:      50m\n",
      "      memory:   64Mi\n",
      "    Liveness:   exec [pgrep -f powercap] delay=30s timeout=10s period=60s #success=1 #failure=3\n",
      "    Readiness:  exec [sh -c test -d /sys/devices/virtual/powercap/intel-rapl && pgrep -f powercap] delay=15s timeout=10s period=30s #success=1 #failure=3\n",
      "    Environment Variables from:\n",
      "      powercap-config  ConfigMap  Optional: false\n",
      "    Environment:\n",
      "      NODE_NAME:   (v1:spec.nodeName)\n",
      "    Mounts:\n",
      "      /app/data from data-storage (rw)\n",
      "      /sys/devices/virtual/powercap from rapl (rw)\n",
      "  Volumes:\n",
      "   rapl:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sys/devices/virtual/powercap\n",
      "    HostPathType:  Directory\n",
      "   data-storage:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/lib/powercap\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  Node-Selectors:  <none>\n",
      "\n",
      "--- 3. Liste des pods powercap-manager sur tous les nœuds ---\n",
      "NAME                     READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES\n",
      "powercap-manager-jb72g   1/1     Running   0          3m48s   172.16.101.9   paradoxe-9   <none>           <none>\n",
      "powercap-manager-tjpl4   1/1     Running   0          3m18s   172.16.101.8   paradoxe-8   <none>           <none>\n",
      "\n",
      "--- 4. Nombre de pods actifs par rapport au nombre attendu ---\n",
      "Pods désirés: 2\n",
      "Pods prêts: 2\n",
      "Pods disponibles: 2\n",
      "\n",
      "--- 5. Description détaillée de chaque pod ---\n",
      "\n",
      "==========================================\n",
      "Pod: powercap-manager-jb72g\n",
      "==========================================\n",
      "\n",
      "--- État général ---\n",
      "NAME                     READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES\n",
      "powercap-manager-jb72g   1/1     Running   0          3m48s   172.16.101.9   paradoxe-9   <none>           <none>\n",
      "\n",
      "--- Nœud d'exécution ---\n",
      "Nœud: paradoxe-9\n",
      "\n",
      "--- Statut du conteneur ---\n",
      "Conteneur: powercap-manager\n",
      "Ready: true\n",
      "Restarts: 0\n",
      "Image: ghcr.io/menraromial/powercap:latest\n",
      "\n",
      "--- Conditions du pod ---\n",
      "PodReadyToStartContainers: True ()\n",
      "Initialized: True ()\n",
      "Ready: True ()\n",
      "ContainersReady: True ()\n",
      "PodScheduled: True ()\n",
      "\n",
      "==========================================\n",
      "Pod: powercap-manager-tjpl4\n",
      "==========================================\n",
      "\n",
      "--- État général ---\n",
      "NAME                     READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES\n",
      "powercap-manager-tjpl4   1/1     Running   0          3m19s   172.16.101.8   paradoxe-8   <none>           <none>\n",
      "\n",
      "--- Nœud d'exécution ---\n",
      "Nœud: paradoxe-8\n",
      "\n",
      "--- Statut du conteneur ---\n",
      "Conteneur: powercap-manager\n",
      "Ready: true\n",
      "Restarts: 0\n",
      "Image: ghcr.io/menraromial/powercap:latest\n",
      "\n",
      "--- Conditions du pod ---\n",
      "PodReadyToStartContainers: True ()\n",
      "Initialized: True ()\n",
      "Ready: True ()\n",
      "ContainersReady: True ()\n",
      "PodScheduled: True ()\n",
      "\n",
      "--- 6. Logs de tous les pods powercap-manager ---\n",
      "\n",
      "==========================================\n",
      "📝 LOGS du pod: powercap-manager-jb72g\n",
      "==========================================\n",
      "[PowerManager] 2025/10/08 12:12:35.670525 Starting professional power management system...\n",
      "[PowerManager] 2025/10/08 12:12:35.672500 Configured data provider: EPEX\n",
      "[PowerManager] 2025/10/08 12:12:35.672510 Discovered 2 RAPL domains\n",
      "[PowerManager] 2025/10/08 12:12:35.672526 Data file epex_data_2025-10-08.csv not found, attempting to generate...\n",
      "[PowerManager] 2025/10/08 12:12:35.672529 Refreshing data for 2025-10-08 using provider EPEX\n",
      "[PowerManager] 2025/10/08 12:12:36.264220 Retrieved 96 data points\n",
      "[PowerManager] 2025/10/08 12:12:36.264517 Successfully refreshed data for 2025-10-08\n",
      "[PowerManager] 2025/10/08 12:12:36.264607 Loaded 96 market data points for 2025-10-08\n",
      "[PowerManager] 2025/10/08 12:12:36.279971 Node already initialized, skipping initialization\n",
      "[PowerManager] 2025/10/08 12:12:36.279996 Power management system ready - starting main cycle\n",
      "[PowerManager] 2025/10/08 12:12:36.280002 Starting power management cycle...\n",
      "[PowerManager] 2025/10/08 12:12:36.280017 Next data refresh scheduled in 11h47m23.719988954s (at 2025-10-09 00:00:00)\n",
      "[PowerManager] 2025/10/08 12:12:36.285150 Power calculation: period=12:00-12:15, source=57878315 µW, max=652000000 µW, min=20000000 µW, applied=57878315 µW\n",
      "\n",
      "==========================================\n",
      "📝 LOGS du pod: powercap-manager-tjpl4\n",
      "==========================================\n",
      "[PowerManager] 2025/10/08 12:13:05.928857 Starting professional power management system...\n",
      "[PowerManager] 2025/10/08 12:13:05.930223 Configured data provider: EPEX\n",
      "[PowerManager] 2025/10/08 12:13:05.930230 Discovered 2 RAPL domains\n",
      "[PowerManager] 2025/10/08 12:13:05.930246 Data file epex_data_2025-10-08.csv not found, attempting to generate...\n",
      "[PowerManager] 2025/10/08 12:13:05.930250 Refreshing data for 2025-10-08 using provider EPEX\n",
      "[PowerManager] 2025/10/08 12:13:06.727773 Retrieved 96 data points\n",
      "[PowerManager] 2025/10/08 12:13:06.728367 Successfully refreshed data for 2025-10-08\n",
      "[PowerManager] 2025/10/08 12:13:06.728439 Loaded 96 market data points for 2025-10-08\n",
      "[PowerManager] 2025/10/08 12:13:06.742775 Node already initialized, skipping initialization\n",
      "[PowerManager] 2025/10/08 12:13:06.742795 Power management system ready - starting main cycle\n",
      "[PowerManager] 2025/10/08 12:13:06.742800 Starting power management cycle...\n",
      "[PowerManager] 2025/10/08 12:13:06.742813 Next data refresh scheduled in 11h46m53.257192608s (at 2025-10-09 00:00:00)\n",
      "[PowerManager] 2025/10/08 12:13:06.746758 Power calculation: period=12:00-12:15, source=57878315 µW, max=652000000 µW, min=20000000 µW, applied=57878315 µW\n",
      "\n",
      "--- 7. Événements récents liés à powercap-manager ---\n",
      "25m         Normal    Started             pod/powercap-manager-2qqb9        Started container powercap-manager\n",
      "25m         Normal    Created             pod/powercap-manager-2qqb9        Created container powercap-manager\n",
      "25m         Normal    Started             pod/powercap-manager-vwrhk        Started container powercap-manager\n",
      "25m         Normal    Created             pod/powercap-manager-2rsqh        Created container powercap-manager\n",
      "25m         Warning   Unhealthy           pod/powercap-manager-2rsqh        Readiness probe errored: rpc error: code = NotFound desc = container is not created or running: checking if PID of 797dbbea273bfaaa34ac91d97b7589ec67ce9e83c6a61726328bcfe34f12a3d3 is running failed: container process not found\n",
      "25m         Normal    Started             pod/powercap-manager-2rsqh        Started container powercap-manager\n",
      "24m         Warning   Unhealthy           pod/powercap-manager-2qqb9        Liveness probe failed: command timed out\n",
      "23m         Warning   Unhealthy           pod/powercap-manager-vwrhk        Liveness probe failed: command timed out\n",
      "23m         Warning   Unhealthy           pod/powercap-manager-2rsqh        Liveness probe failed: command timed out\n",
      "18m         Warning   Unhealthy           pod/powercap-manager-2qqb9        Readiness probe failed: command timed out\n",
      "18m         Warning   Unhealthy           pod/powercap-manager-2rsqh        Readiness probe failed: command timed out\n",
      "13m         Warning   Unhealthy           pod/powercap-manager-vwrhk        Readiness probe failed: command timed out\n",
      "3m50s       Normal    Killing             pod/powercap-manager-2qqb9        Stopping container powercap-manager\n",
      "3m50s       Normal    SuccessfulDelete    daemonset/powercap-manager        Deleted pod: powercap-manager-vwrhk\n",
      "3m50s       Normal    SuccessfulDelete    daemonset/powercap-manager        Deleted pod: powercap-manager-2qqb9\n",
      "3m50s       Normal    Killing             pod/powercap-manager-vwrhk        Stopping container powercap-manager\n",
      "3m49s       Normal    Pulling             pod/powercap-manager-jb72g        Pulling image \"ghcr.io/menraromial/powercap:latest\"\n",
      "3m49s       Normal    Scheduled           pod/powercap-manager-jb72g        Successfully assigned default/powercap-manager-jb72g to paradoxe-9\n",
      "3m49s       Normal    SuccessfulCreate    daemonset/powercap-manager        Created pod: powercap-manager-jb72g\n",
      "3m47s       Normal    Created             pod/powercap-manager-jb72g        Created container powercap-manager\n",
      "3m47s       Normal    Pulled              pod/powercap-manager-jb72g        Successfully pulled image \"ghcr.io/menraromial/powercap:latest\" in 2.103s (2.103s including waiting). Image size: 44622807 bytes.\n",
      "3m47s       Normal    Started             pod/powercap-manager-jb72g        Started container powercap-manager\n",
      "3m19s       Normal    Scheduled           pod/powercap-manager-tjpl4        Successfully assigned default/powercap-manager-tjpl4 to paradoxe-8\n",
      "3m19s       Normal    Killing             pod/powercap-manager-2rsqh        Stopping container powercap-manager\n",
      "3m19s       Normal    SuccessfulDelete    daemonset/powercap-manager        Deleted pod: powercap-manager-2rsqh\n",
      "3m19s       Normal    SuccessfulCreate    daemonset/powercap-manager        Created pod: powercap-manager-tjpl4\n",
      "3m18s       Normal    Pulling             pod/powercap-manager-tjpl4        Pulling image \"ghcr.io/menraromial/powercap:latest\"\n",
      "3m17s       Normal    Pulled              pod/powercap-manager-tjpl4        Successfully pulled image \"ghcr.io/menraromial/powercap:latest\" in 1.635s (1.635s including waiting). Image size: 44622807 bytes.\n",
      "3m17s       Normal    Created             pod/powercap-manager-tjpl4        Created container powercap-manager\n",
      "3m17s       Normal    Started             pod/powercap-manager-tjpl4        Started container powercap-manager\n",
      "\n",
      "--- 8. Vérification de l'accès aux ressources système ---\n",
      "Test d'accès RAPL sur le pod: powercap-manager-jb72g\n",
      "total 0\n",
      "drwxr-xr-x    4 root     root             0 Oct  8 10:54 .\n",
      "drwxr-xr-x   19 root     root             0 Oct  8 10:54 ..\n",
      "drwxr-xr-x    3 root     root             0 Oct  8 10:54 dtpm\n",
      "drwxr-xr-x    5 root     root             0 Oct  8 10:54 intel-rapl\n",
      "\n",
      "--- 9. Vérification de la ConfigMap ---\n",
      "apiVersion: v1\n",
      "data:\n",
      "  ALPHA: \"4\"\n",
      "  DATA_PROVIDER: epex\n",
      "  DATA_REFRESH_CRON: 0 0 * * *\n",
      "  MAX_SOURCE: \"100000000\"\n",
      "  PROVIDER_PARAMS: |\n",
      "    {\n",
      "      \"market_area\": \"FR\",\n",
      "      \"auction\": \"IDA1\",\n",
      "      \"modality\": \"Auction\",\n",
      "      \"sub_modality\": \"Intraday\",\n",
      "      \"data_mode\": \"table\"\n",
      "    }\n",
      "  PROVIDER_URL: https://www.epexspot.com/en/market-results\n",
      "  RAPL_MIN_POWER: \"20000000\"\n",
      "  STABILISATION_TIME: \"300\"\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  annotations:\n",
      "    kubectl.kubernetes.io/last-applied-configuration: |\n",
      "      {\"apiVersion\":\"v1\",\"data\":{\"ALPHA\":\"4\",\"DATA_PROVIDER\":\"epex\",\"DATA_REFRESH_CRON\":\"0 0 * * *\",\"MAX_SOURCE\":\"100000000\",\"PROVIDER_PARAMS\":\"{\\n  \\\"market_area\\\": \\\"FR\\\",\\n  \\\"auction\\\": \\\"IDA1\\\",\\n  \\\"modality\\\": \\\"Auction\\\",\\n  \\\"sub_modality\\\": \\\"Intraday\\\",\\n  \\\"data_mode\\\": \\\"table\\\"\\n}\\n\",\"PROVIDER_URL\":\"https://www.epexspot.com/en/market-results\",\"RAPL_MIN_POWER\":\"20000000\",\"STABILISATION_TIME\":\"300\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"power-management\",\"app.kubernetes.io/name\":\"powercap-manager\",\"app.kubernetes.io/version\":\"v1.0.0\"},\"name\":\"powercap-config\",\"namespace\":\"default\"}}\n",
      "  creationTimestamp: \"2025-10-08T11:47:36Z\"\n",
      "  labels:\n",
      "    app.kubernetes.io/component: power-management\n",
      "    app.kubernetes.io/name: powercap-manager\n",
      "    app.kubernetes.io/version: v1.0.0\n",
      "  name: powercap-config\n",
      "  namespace: default\n",
      "  resourceVersion: \"7095\"\n",
      "  uid: b066af72-f6d2-43f3-8a23-1036210927ec\n",
      "\n",
      "--- 10. Vérification des erreurs critiques ---\n",
      "\n",
      "========================================================================\n",
      "✅ Vérification terminée\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "# Script pour vérifier l'état détaillé et afficher les logs\n",
    "CHECK_POWERCAP_STATUS_LOGS = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"📊 VÉRIFICATION DÉTAILLÉE - État et Logs PowerCap Manager\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 1. État du DaemonSet powercap-manager ---\"\n",
    "kubectl get daemonset powercap-manager -n default -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 2. Détails du DaemonSet ---\"\n",
    "kubectl describe daemonset powercap-manager -n default | head -50\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 3. Liste des pods powercap-manager sur tous les nœuds ---\"\n",
    "kubectl get pods -n default -l app=powercap-manager -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 4. Nombre de pods actifs par rapport au nombre attendu ---\"\n",
    "DESIRED=$(kubectl get daemonset powercap-manager -n default -o jsonpath='{.status.desiredNumberScheduled}')\n",
    "READY=$(kubectl get daemonset powercap-manager -n default -o jsonpath='{.status.numberReady}')\n",
    "AVAILABLE=$(kubectl get daemonset powercap-manager -n default -o jsonpath='{.status.numberAvailable}')\n",
    "echo \"Pods désirés: $DESIRED\"\n",
    "echo \"Pods prêts: $READY\"\n",
    "echo \"Pods disponibles: $AVAILABLE\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 5. Description détaillée de chaque pod ---\"\n",
    "POWERCAP_PODS=$(kubectl get pods -n default -l app=powercap-manager -o jsonpath='{.items[*].metadata.name}')\n",
    "\n",
    "if [ -n \"$POWERCAP_PODS\" ]; then\n",
    "    for pod in $POWERCAP_PODS; do\n",
    "        echo \"\"\n",
    "        echo \"==========================================\"\n",
    "        echo \"Pod: $pod\"\n",
    "        echo \"==========================================\"\n",
    "        \n",
    "        echo \"\"\n",
    "        echo \"--- État général ---\"\n",
    "        kubectl get pod $pod -n default -o wide\n",
    "        \n",
    "        echo \"\"\n",
    "        echo \"--- Nœud d'exécution ---\"\n",
    "        NODE=$(kubectl get pod $pod -n default -o jsonpath='{.spec.nodeName}')\n",
    "        echo \"Nœud: $NODE\"\n",
    "        \n",
    "        echo \"\"\n",
    "        echo \"--- Statut du conteneur ---\"\n",
    "        kubectl get pod $pod -n default -o jsonpath='{range .status.containerStatuses[*]}Conteneur: {.name}\n",
    "Ready: {.ready}\n",
    "Restarts: {.restartCount}\n",
    "Image: {.image}\n",
    "{end}'\n",
    "        \n",
    "        echo \"\"\n",
    "        echo \"--- Conditions du pod ---\"\n",
    "        kubectl get pod $pod -n default -o jsonpath='{range .status.conditions[*]}{.type}: {.status} ({.reason})\n",
    "{end}'\n",
    "    done\n",
    "else\n",
    "    echo \"❌ Aucun pod powercap-manager trouvé dans le namespace default\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 6. Logs de tous les pods powercap-manager ---\"\n",
    "if [ -n \"$POWERCAP_PODS\" ]; then\n",
    "    for pod in $POWERCAP_PODS; do\n",
    "        echo \"\"\n",
    "        echo \"==========================================\"\n",
    "        echo \"📝 LOGS du pod: $pod\"\n",
    "        echo \"==========================================\"\n",
    "        kubectl logs $pod -n default --tail=100 2>&1 || echo \"Impossible de récupérer les logs\"\n",
    "    done\n",
    "else\n",
    "    echo \"❌ Aucun log à afficher\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 7. Événements récents liés à powercap-manager ---\"\n",
    "kubectl get events -n default --sort-by='.lastTimestamp' | grep powercap | tail -30 || echo \"Aucun événement récent\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 8. Vérification de l'accès aux ressources système ---\"\n",
    "if [ -n \"$POWERCAP_PODS\" ]; then\n",
    "    # Test sur le premier pod\n",
    "    FIRST_POD=$(echo $POWERCAP_PODS | awk '{print $1}')\n",
    "    echo \"Test d'accès RAPL sur le pod: $FIRST_POD\"\n",
    "    kubectl exec $FIRST_POD -n default -- ls -la /sys/devices/virtual/powercap/ 2>&1 || echo \"❌ Impossible d'accéder au répertoire RAPL\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 9. Vérification de la ConfigMap ---\"\n",
    "kubectl get configmap powercap-config -n default -o yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 10. Vérification des erreurs critiques ---\"\n",
    "if [ -n \"$POWERCAP_PODS\" ]; then\n",
    "    for pod in $POWERCAP_PODS; do\n",
    "        STATUS=$(kubectl get pod $pod -n default -o jsonpath='{.status.phase}')\n",
    "        if [ \"$STATUS\" != \"Running\" ]; then\n",
    "            echo \"\"\n",
    "            echo \"⚠️  ATTENTION: Pod $pod est en état: $STATUS\"\n",
    "            echo \"Raison:\"\n",
    "            kubectl get pod $pod -n default -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].message}'\n",
    "            echo \"\"\n",
    "            echo \"Logs d'erreur:\"\n",
    "            kubectl logs $pod -n default --tail=20 2>&1\n",
    "        fi\n",
    "    done\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"✅ Vérification terminée\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 Lancement de la vérification détaillée de l'état et des logs PowerCap...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(CHECK_POWERCAP_STATUS_LOGS)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ef55d",
   "metadata": {},
   "source": [
    "## Suppression du DaemonSet PowerCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a85425d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea550fcdab694226a2e7492ec3e483ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Vérification/copie du répertoire 'powercap' vers le master...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">copy</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mcopy\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d2e1424ea1490ca4ab4f7b966f337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Répertoire copié/mis à jour sur le master.\n",
      "🗑️  Lancement de la suppression de PowerCap...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-7.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-7.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c049a34d8bb407a99f0c58f596685e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "🗑️  SUPPRESSION des Ressources PowerCap\n",
      "========================================================================\n",
      "\n",
      "--- Suppression de toutes les ressources via les manifestes ---\n",
      "configmap \"powercap-config\" deleted\n",
      "daemonset.apps \"powercap-manager\" deleted\n",
      "serviceaccount \"powercap-manager\" deleted\n",
      "clusterrole.rbac.authorization.k8s.io \"powercap-manager\" deleted\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"powercap-manager\" deleted\n",
      "\n",
      "--- Attente de la suppression des pods (10 secondes) ---\n",
      "\n",
      "--- Vérification finale ---\n",
      "\n",
      "DaemonSets restants avec 'powercap' :\n",
      "✅ Aucun DaemonSet trouvé\n",
      "\n",
      "Pods restants avec 'powercap' :\n",
      "\n",
      "ConfigMaps restantes avec 'powercap' :\n",
      "✅ Aucune ConfigMap trouvée\n",
      "\n",
      "ServiceAccounts restants avec 'powercap' :\n",
      "✅ Aucun ServiceAccount trouvé\n",
      "\n",
      "ClusterRoles restants avec 'powercap' :\n",
      "✅ Aucun ClusterRole trouvé\n",
      "\n",
      "ClusterRoleBindings restants avec 'powercap' :\n",
      "✅ Aucun ClusterRoleBinding trouvé\n",
      "\n",
      "========================================================================\n",
      "✅ Suppression de PowerCap terminée !\n",
      "========================================================================\n",
      "\n",
      "--- Nettoyage des données sur les nœuds workers ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Finished </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> tasks</span> <span style=\"font-weight: bold; font-style: italic\">(</span><span style=\"font-style: italic\">shell</span><span style=\"font-weight: bold; font-style: italic\">)</span> on <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-9.rennes.grid5000.fr'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'paradoxe-8.rennes.grid5000.fr'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mFinished \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m tasks\u001b[0m \u001b[1;3m(\u001b[0m\u001b[3mshell\u001b[0m\u001b[1;3m)\u001b[0m on \u001b[1m{\u001b[0m\u001b[32m'paradoxe-9.rennes.grid5000.fr'\u001b[0m, \n",
       "\u001b[32m'paradoxe-8.rennes.grid5000.fr'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Toutes les ressources PowerCap ont été supprimées du cluster !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Vérifier si le dossier local existe, sinon afficher une erreur\n",
    "source_dir = \"powercap\"\n",
    "dest_dir = \"/tmp/powercap\"\n",
    "\n",
    "if not os.path.isdir(source_dir):\n",
    "    print(f\"❌ Le répertoire local '{source_dir}' n'existe pas.\")\n",
    "    print(\"💡 Créez le dossier avec les manifestes avant de supprimer les ressources.\")\n",
    "    raise FileNotFoundError(f\"Le répertoire '{source_dir}' est introuvable.\")\n",
    "\n",
    "# --- Copier le répertoire vers le master (au cas où il n'existe plus sur le master) ---\n",
    "print(f\"📁 Vérification/copie du répertoire '{source_dir}' vers le master...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=source_dir, dest=\"/tmp/\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"✅ Répertoire copié/mis à jour sur le master.\")\n",
    "\n",
    "# Script pour supprimer complètement le DaemonSet PowerCap en utilisant les manifestes\n",
    "DELETE_POWERCAP_SCRIPT = f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"🗑️  SUPPRESSION des Ressources PowerCap\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Suppression de toutes les ressources via les manifestes ---\"\n",
    "kubectl delete -f {dest_dir}/ --ignore-not-found=true\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente de la suppression des pods (10 secondes) ---\"\n",
    "sleep 10\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Vérification finale ---\"\n",
    "echo \"\"\n",
    "echo \"DaemonSets restants avec 'powercap' :\"\n",
    "kubectl get daemonsets -n default | grep powercap || echo \"✅ Aucun DaemonSet trouvé\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Pods restants avec 'powercap' :\"\n",
    "kubectl get pods -n default -l app=powercap-manager || echo \"✅ Aucun pod trouvé\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"ConfigMaps restantes avec 'powercap' :\"\n",
    "kubectl get configmap -n default | grep powercap || echo \"✅ Aucune ConfigMap trouvée\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"ServiceAccounts restants avec 'powercap' :\"\n",
    "kubectl get serviceaccount -n default | grep powercap || echo \"✅ Aucun ServiceAccount trouvé\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"ClusterRoles restants avec 'powercap' :\"\n",
    "kubectl get clusterrole | grep powercap || echo \"✅ Aucun ClusterRole trouvé\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"ClusterRoleBindings restants avec 'powercap' :\"\n",
    "kubectl get clusterrolebinding | grep powercap || echo \"✅ Aucun ClusterRoleBinding trouvé\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"✅ Suppression de PowerCap terminée !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"🗑️  Lancement de la suppression de PowerCap...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DELETE_POWERCAP_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)\n",
    "\n",
    "# --- Nettoyage des données sur les workers ---\n",
    "print(\"\\n--- Nettoyage des données sur les nœuds workers ---\")\n",
    "CLEANUP_WORKERS_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "echo \"Suppression des données PowerCap dans /var/lib/powercap...\"\n",
    "sudo rm -rf /var/lib/powercap/*.csv 2>/dev/null || true\n",
    "echo \"✅ Nettoyage terminé sur ce nœud\"\n",
    "\"\"\"\n",
    "\n",
    "with en.actions(roles=worker_nodes) as p:\n",
    "    p.shell(CLEANUP_WORKERS_SCRIPT)\n",
    "\n",
    "print(\"✅ Toutes les ressources PowerCap ont été supprimées du cluster !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797649d",
   "metadata": {},
   "source": [
    "# Libération des Ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf720446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libération des ressources sur Grid'5000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:enoslib.log:[G5k] Reloading 3292322 from rennes\n",
      "INFO:enoslib.log:[G5k] Killing the job (rennes, 3292322)\n",
      "INFO:enoslib.log:[G5k] Killing the job (rennes, 3292322)\n",
      "INFO:enoslib.log:[G5k] Job killed (rennes, 3292322)\n",
      "INFO:enoslib.log:[G5k] Job killed (rennes, 3292322)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ressources libérées. ✅\n"
     ]
    }
   ],
   "source": [
    "# Destruction de la réservation\n",
    "print(\"Libération des ressources sur Grid'5000...\")\n",
    "provider.destroy()\n",
    "print(\"Ressources libérées. ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
