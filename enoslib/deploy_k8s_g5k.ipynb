{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6f19e1",
   "metadata": {},
   "source": [
    "# Importations et Configuration Initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60918c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enoslib as en\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# --- Configuration de l'exp√©rience ---\n",
    "\n",
    "# Configurez le logging pour avoir un retour clair sur les √©tapes\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "G5K_SITE = \"rennes\"  # Site Grid'5000 √† utiliser\n",
    "\n",
    "# Nom du job pour la r√©servation sur Grid'5000\n",
    "JOB_NAME = \"K8sEnoslibDeploy\"\n",
    "\n",
    "# Cl√© SSH, par d√©faut enoslib cherche ~/.ssh/id_rsa\n",
    "# Si vous en utilisez une autre, d√©commentez et modifiez la ligne suivante :\n",
    "# en.set_config(ssh_key=\"~/.ssh/ma_cle_ssh\")\n",
    "\n",
    "# Cr√©ation du chemin pour les sorties (par exemple, inventaire)\n",
    "Path(\"inventory\").mkdir(exist_ok=True)\n",
    "OUTPUT_INVENTORY = \"inventory/g5k_inventory.yaml\"\n",
    "\n",
    "print(\"Configuration initiale charg√©e.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db030d",
   "metadata": {},
   "source": [
    "# D√©finition des Ressources pour le Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89826ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 2\n",
    "# D√©finition de la configuration pour la connexion √† Grid'5000\n",
    "# en.set_provider(\n",
    "#     en.G5k(\n",
    "#         username=G5K_USER,\n",
    "#         site=G5K_SITE,\n",
    "#         job_name=JOB_NAME,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# D√©finition des ressources √† r√©server\n",
    "# 1 n≈ìud master et 2 n≈ìuds workers\n",
    "# Tous les n≈ìuds seront d√©ploy√©s avec Ubuntu 22.04\n",
    "conf = (\n",
    "en.G5kConf.from_settings(\n",
    "    job_name=JOB_NAME,\n",
    "    job_type=[\"deploy\"],\n",
    "    walltime=\"03:30:00\", # R√©servez pour 1 heure\n",
    "    env_name=\"ubuntu2204-min\"\n",
    ").add_machine(\n",
    "    roles=[\"master\"],\n",
    "    cluster=\"paradoxe\",\n",
    "    nodes=1\n",
    ").add_machine(\n",
    "    roles=[\"workers\"],\n",
    "    cluster=\"paradoxe\",\n",
    "    nodes=2\n",
    ")\n",
    ")\n",
    "\n",
    "print(\"D√©finition des ressources termin√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7476000e",
   "metadata": {},
   "source": [
    "# Ex√©cution de la R√©servation et du D√©ploiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "provider = en.G5k(conf)\n",
    "# D√©marrage de la r√©servation et du d√©ploiement\n",
    "# provider.init() bloque jusqu'√† ce que les noeuds soient pr√™ts\n",
    "\n",
    "roles, networks = provider.init()\n",
    "\n",
    "# R√©cup√©ration des informations sur les r√¥les pour les cellules suivantes\n",
    "#roles = reservation.get_roles()\n",
    "master_node = roles[\"master\"]\n",
    "worker_nodes = roles[\"workers\"]\n",
    "\n",
    "print(\"--- R√©servation et d√©ploiement termin√©s ! ---\")\n",
    "print(f\"Master: {[node.address for node in master_node]}\")\n",
    "print(f\"Workers: {[node.address for node in worker_nodes]}\")\n",
    "#print(f\"Tous les n≈ìuds: roles\" + str(roles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f16248",
   "metadata": {},
   "source": [
    "#  Pr√©paration Commune de Tous les N≈ìuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script de configuration commun\n",
    "COMMON_SETUP_SCRIPT = \"\"\"\n",
    "#!/bin/bash -xe\n",
    "sudo apt-get update -y\n",
    "sudo apt-get install -y software-properties-common gpg curl apt-transport-https ca-certificates jq\n",
    "cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf\n",
    "overlay\n",
    "br_netfilter\n",
    "EOF\n",
    "sudo modprobe overlay\n",
    "sudo modprobe br_netfilter\n",
    "cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\n",
    "net.bridge.bridge-nf-call-iptables  = 1\n",
    "net.bridge.bridge-nf-call-ip6tables = 1\n",
    "net.ipv4.ip_forward                 = 1\n",
    "EOF\n",
    "sudo sysctl --system\n",
    "sudo swapoff -a\n",
    "curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key | gpg --dearmor | sudo tee /etc/apt/keyrings/cri-o-apt-keyring.gpg >/dev/null\n",
    "echo \"deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /\" | sudo tee /etc/apt/sources.list.d/cri-o.list\n",
    "sudo apt-get update -y\n",
    "sudo apt-get install -y cri-o\n",
    "sudo systemctl enable --now crio\n",
    "KUBERNETES_VERSION=\"1.30\"\n",
    "sudo mkdir -p /etc/apt/keyrings\n",
    "curl -fsSL https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n",
    "echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/ /\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y kubelet=1.30.0-1.1 kubectl=1.30.0-1.1 kubeadm=1.30.0-1.1\n",
    "sudo apt-mark hold kubelet kubeadm kubectl\n",
    "IFACE=$(ip route | grep default | awk '{print $5}')\n",
    "local_ip=$(ip --json addr show $IFACE | jq -r '.[0].addr_info[] | select(.family == \"inet\") | .local')\n",
    "echo \"KUBELET_EXTRA_ARGS=--node-ip=$local_ip\" | sudo tee /etc/default/kubelet\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution du script sur tous les n≈ìuds\n",
    "print(\"Lancement de la pr√©paration commune...\")\n",
    "with en.actions(roles=roles) as p:\n",
    "    p.shell(COMMON_SETUP_SCRIPT)\n",
    "print(\"Pr√©paration commune termin√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18b04e",
   "metadata": {},
   "source": [
    "##  V√©rification de la Pr√©paration des N≈ìuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script de v√©rification (inchang√©)\n",
    "VERIFY_PREP_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "echo \"--- V√©rification de l'√©tat des n≈ìuds ---\"\n",
    "echo \"[V√âRIF] Service CRI-O (runtime de conteneurs)...\"\n",
    "sudo systemctl is-active --quiet crio\n",
    "echo \"‚úÖ OK: Le service CRI-O est actif.\"\n",
    "echo \"[V√âRIF] Binaires Kubernetes...\"\n",
    "kubelet --version\n",
    "kubeadm version\n",
    "echo \"‚úÖ OK: Les binaires Kubelet et Kubeadm sont install√©s.\"\n",
    "echo \"[V√âRIF] Modules Kernel...\"\n",
    "lsmod | grep -q br_netfilter && echo \"‚úÖ OK: Module 'br_netfilter' est charg√©.\"\n",
    "lsmod | grep -q overlay && echo \"‚úÖ OK: Module 'overlay' est charg√©.\"\n",
    "echo \"\\nüéâ La pr√©paration de ce n≈ìud est valid√©e.\"\n",
    "\"\"\"\n",
    "\n",
    "# 1. On ex√©cute les actions (comme avant)\n",
    "print(\"Lancement de la v√©rification sur tous les n≈ìuds...\")\n",
    "with en.actions(roles=roles) as p:\n",
    "    p.shell(VERIFY_PREP_SCRIPT)\n",
    "\n",
    "results = p.results\n",
    "verify = results[0].stdout.strip()\n",
    "print(verify)\n",
    "# # 2. ‚úÖ NOUVEAU : On r√©cup√®re et on affiche les r√©sultats APR√àS le bloc\n",
    "# print(\"\\n--- R√âSULTATS DE LA V√âRIFICATION ---\")\n",
    "# results = p.get_results()\n",
    "\n",
    "# # On boucle sur les r√©sultats de chaque machine\n",
    "# for r in results:\n",
    "#     print(f\"\\n======== N≈ìud: {r.host} ========\")\n",
    "#     print(r.stdout) # Affiche la sortie standard de la commande\n",
    "    \n",
    "#     # Optionnel : Affiche les erreurs s'il y en a\n",
    "#     if r.stderr:\n",
    "#         print(f\"-------- Erreurs (stderr) pour {r.host} --------\")\n",
    "#         print(r.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3926281",
   "metadata": {},
   "source": [
    "#  Initialisation du Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script d'initialisation du master\n",
    "MASTER_INIT_SCRIPT = \"\"\"\n",
    "#!/bin/bash -xe\n",
    "IFACE=$(ip route | grep default | awk '{print $5}')\n",
    "IPADDR=$(ip -4 addr show $IFACE | grep -oP '(?<=inet\\s)\\d+(\\.\\d+){3}')\n",
    "NODENAME=$(hostname -s)\n",
    "sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --apiserver-advertise-address=$IPADDR --node-name $NODENAME --cri-socket unix:///var/run/crio/crio.sock --ignore-preflight-errors=Swap\n",
    "mkdir -p $HOME/.kube\n",
    "sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n",
    "sudo chown $(id -u):$(id -g) $HOME/.kube/config\n",
    "kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution du script sur le master\n",
    "print(\"Initialisation du master...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(MASTER_INIT_SCRIPT)\n",
    "print(\"Initialisation du master termin√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079e4de",
   "metadata": {},
   "source": [
    "# Jonction des Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. R√©cup√©rer la commande de jonction depuis le master\n",
    "print(\"R√©cup√©ration de la commande de jonction depuis le master...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(\"sudo kubeadm token create --print-join-command\")\n",
    "\n",
    "# R√©cup√©ration des r√©sultats apr√®s le bloc\n",
    "results = p.results\n",
    "join_command = results[0].stdout.strip()\n",
    "print(f\"‚úÖ Commande de jonction r√©cup√©r√©e: {join_command}\")\n",
    "\n",
    "# 2. Ex√©cuter la commande sur les workers\n",
    "print(\"Les workers rejoignent le cluster...\")\n",
    "with en.actions(roles=worker_nodes) as p:\n",
    "    p.shell(f\"sudo {join_command}\")\n",
    "\n",
    "print(\"Workers joints. Attente de 60s pour la stabilisation...\")\n",
    "#time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a921f89",
   "metadata": {},
   "source": [
    "# V√©rification du Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7195aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cution de kubectl get nodes sur le master pour v√©rifier\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(\"kubectl get nodes -o wide\")\n",
    "\n",
    "results = p.results\n",
    "nodes = results[0].stdout.strip()\n",
    "print(\"--- √âtat du cluster ---\")\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524c748",
   "metadata": {},
   "source": [
    " ## Installation des Add-ons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour installer les add-ons\n",
    "ADDONS_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- Installation du Metrics Server ---\"\n",
    "kubectl apply -f https://raw.githubusercontent.com/techiescamp/kubeadm-scripts/main/manifests/metrics-server.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Installation d'Ingress-NGINX ---\"\n",
    "kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.13.0/deploy/static/provider/baremetal/deploy.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"Installation des add-ons termin√©e.\"\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution du script sur le master\n",
    "print(\"Installation des add-ons sur le n≈ìud master...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(ADDONS_SCRIPT)\n",
    "\n",
    "# R√©cup√©ration de la sortie comme vous l'avez sp√©cifi√©\n",
    "print(\"\\n--- R√©sultat de l'installation ---\")\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"Aucun r√©sultat √† afficher. V√©rifiez les erreurs potentielles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b258c",
   "metadata": {},
   "source": [
    "# D√©ploiement et Test d'une Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour d√©ployer l'application de test et son service\n",
    "APP_DEPLOY_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- Cr√©ation du Deployment et du Service pour l'application echoserver ---\"\n",
    "\n",
    "# On utilise un 'heredoc' pour passer le YAML directement √† kubectl\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: echoserver\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: echoserver\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: echoserver\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: echoserver\n",
    "        image: registry.k8s.io/echoserver:1.10\n",
    "        ports:\n",
    "        - containerPort: 8080\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: echoserver\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    app: echoserver\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080\n",
    "EOF\n",
    "\n",
    "echo \"\\n--- Attente de 15 secondes pour le d√©marrage des pods ---\"\n",
    "sleep 15\n",
    "\n",
    "echo \"\\n--- V√©rification du statut du d√©ploiement ---\"\n",
    "kubectl get deployment echoserver\n",
    "\n",
    "echo \"\\n--- V√©rification des pods (devrait en afficher 2) ---\"\n",
    "kubectl get pods -l app=echoserver\n",
    "\n",
    "echo \"\\n--- V√©rification du service (notez le port apr√®s '80:') ---\"\n",
    "kubectl get service echoserver\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution du script sur le master\n",
    "print(\"D√©ploiement de l'application de test...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(APP_DEPLOY_SCRIPT)\n",
    "\n",
    "# Affichage du r√©sultat\n",
    "print(\"\\n--- R√©sultat du d√©ploiement ---\")\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c83c8",
   "metadata": {},
   "source": [
    "### Envoyez une Requ√™te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e8487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√®re l'adresse IP du master\n",
    "master_ip = master_node[0].address \n",
    "\n",
    "# Remplacez 31192 par le port que vous avez obtenu √† l'√©tape 1\n",
    "node_port = 30968 # ‚ö†Ô∏è CHANGEZ CE PORT\n",
    "\n",
    "# Test avec curl depuis le master\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(f\"curl http://{master_ip}:{node_port}\")\n",
    "\n",
    "if p.results:\n",
    "    print(\"--- R√©ponse du serveur ---\")\n",
    "    print(p.results[0].stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4fcd5b",
   "metadata": {},
   "source": [
    "# Installation de la Suite de Monitoring (Prometheus & Grafana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad67271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour d√©ployer la suite de monitoring\n",
    "MONITORING_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- Installation de la suite Prometheus + Grafana via kube-prometheus ---\"\n",
    "\n",
    "# 1. Cloner le d√©p√¥t qui contient les manifestes\n",
    "if [ ! -d \"kube-prometheus\" ]; then\n",
    "    echo \"Clonage du d√©p√¥t kube-prometheus...\"\n",
    "    git clone https://github.com/prometheus-operator/kube-prometheus.git\n",
    "else\n",
    "    echo \"D√©p√¥t kube-prometheus d√©j√† pr√©sent.\"\n",
    "fi\n",
    "cd kube-prometheus\n",
    "\n",
    "# 2. Appliquer les d√©finitions de ressources (CRDs) et la configuration de base\n",
    "echo \"√âtape 1/3 : Application des CRDs et de la configuration 'setup'...\"\n",
    "kubectl apply --server-side -f manifests/setup\n",
    "\n",
    "# Attendre que les CRDs soient bien enregistr√©s dans le cluster avant de continuer\n",
    "echo \"Attente de l'√©tablissement des CRDs...\"\n",
    "kubectl wait --for condition=Established --all CustomResourceDefinition --timeout=300s\n",
    "\n",
    "# 3. Appliquer le reste de la suite (Prometheus, Grafana, Alertmanager, etc.)\n",
    "echo \"√âtape 2/3 : Application des manifestes de la suite de monitoring...\"\n",
    "kubectl apply -f manifests/\n",
    "\n",
    "# 4. Attendre que les d√©ploiements cl√©s soient pr√™ts dans le namespace 'monitoring'\n",
    "echo \"√âtape 3/3 : Attente du d√©marrage de Prometheus et Grafana (peut prendre quelques minutes)...\"\n",
    "kubectl wait --for=condition=available deployment/prometheus-k8s -n monitoring --timeout=300s\n",
    "kubectl wait --for=condition=available deployment/grafana -n monitoring --timeout=300s\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des pods dans le namespace 'monitoring' ---\"\n",
    "kubectl get pods -n monitoring\n",
    "\n",
    "echo \"\\nüéâ Suite de monitoring Prometheus et Grafana install√©e avec succ√®s !\"\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution du script sur le master\n",
    "print(\"Lancement de l'installation de la suite de monitoring...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(MONITORING_SCRIPT)\n",
    "\n",
    "# Affichage du r√©sultat\n",
    "print(\"\\n--- R√©sultat de l'installation ---\")\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e8a1d",
   "metadata": {},
   "source": [
    "## V√©rification des Composants de Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour lister les pods et services de monitoring\n",
    "VERIFY_MONITORING_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"--- Pods dans le namespace 'monitoring' ---\"\n",
    "kubectl get pods -n monitoring -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Services dans le namespace 'monitoring' ---\"\n",
    "kubectl get services -n monitoring\n",
    "echo \"\"\n",
    "echo \"--- ServiceAccounts dans le namespace 'monitoring' ---\"\n",
    "kubectl get serviceaccounts -n monitoring\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution du script de v√©rification sur le master\n",
    "print(\"R√©cup√©ration de l'√©tat des composants de monitoring...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(VERIFY_MONITORING_SCRIPT)\n",
    "\n",
    "# Affichage du r√©sultat\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab78fdd",
   "metadata": {},
   "source": [
    "## Mise √† Jour des NetworkPolicies pour l'Acc√®s Externe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34266c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour mettre √† jour les NetworkPolicies\n",
    "UPDATE_NP_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- 1. Mise √† jour de la NetworkPolicy de Grafana ---\"\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: grafana\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    app.kubernetes.io/component: grafana\n",
    "    app.kubernetes.io/name: grafana\n",
    "    app.kubernetes.io/part-of: kube-prometheus\n",
    "    app.kubernetes.io/version: 12.2.0\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/component: grafana\n",
    "      app.kubernetes.io/name: grafana\n",
    "      app.kubernetes.io/part-of: kube-prometheus\n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  - Egress\n",
    "  ingress:\n",
    "  - from:\n",
    "    # On autorise le trafic depuis n'importe quelle adresse IP (externe/interne)\n",
    "    - ipBlock:\n",
    "        cidr: 0.0.0.0/0\n",
    "    # On garde la r√®gle existante qui autorise Prometheus\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app.kubernetes.io/name: prometheus\n",
    "    ports:\n",
    "    - port: 3000\n",
    "      protocol: TCP\n",
    "  egress:\n",
    "  - {}\n",
    "EOF\n",
    "\n",
    "echo \"\\n--- 2. Mise √† jour de la NetworkPolicy de Prometheus ---\"\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: prometheus-k8s\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    app.kubernetes.io/component: prometheus\n",
    "    app.kubernetes.io/instance: k8s\n",
    "    app.kubernetes.io/name: prometheus\n",
    "    app.kubernetes.io/part-of: kube-prometheus\n",
    "    app.kubernetes.io/version: 3.6.0\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app.kubernetes.io/component: prometheus\n",
    "      app.kubernetes.io/instance: k8s\n",
    "      app.kubernetes.io/name: prometheus\n",
    "      app.kubernetes.io/part-of: kube-prometheus\n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  - Egress\n",
    "  ingress:\n",
    "  - from:\n",
    "    # On autorise le trafic depuis n'importe quelle adresse IP\n",
    "    - ipBlock:\n",
    "        cidr: 0.0.0.0/0\n",
    "    # On garde les r√®gles existantes pour la communication interne\n",
    "    - namespaceSelector: {}\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app.kubernetes.io/name: prometheus\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app.kubernetes.io/name: prometheus-adapter\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app.kubernetes.io/name: grafana\n",
    "    ports:\n",
    "    - port: 9090\n",
    "      protocol: TCP\n",
    "    - port: 8080\n",
    "      protocol: TCP\n",
    "  egress:\n",
    "  - {}\n",
    "EOF\n",
    "\n",
    "echo \"\\nNetworkPolicies mises √† jour avec succ√®s pour autoriser l'acc√®s externe.\"\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution du script sur le master\n",
    "print(\"Mise √† jour des NetworkPolicies...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(UPDATE_NP_SCRIPT)\n",
    "\n",
    "# Affichage du r√©sultat\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306d0cb",
   "metadata": {},
   "source": [
    "## Passage des Services en NodePort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4963c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour patcher les services en NodePort\n",
    "PATCH_SERVICES_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- 1. Modification du service Grafana en type NodePort ---\"\n",
    "kubectl patch service grafana -n monitoring -p '{\"spec\": {\"type\": \"NodePort\"}}'\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 2. Modification du service Prometheus en type NodePort ---\"\n",
    "kubectl patch service prometheus-k8s -n monitoring -p '{\"spec\": {\"type\": \"NodePort\"}}'\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 3. V√©rification du r√©sultat ---\"\n",
    "# Petite pause pour s'assurer que l'API server a bien trait√© les changements\n",
    "sleep 2\n",
    "kubectl get services -n monitoring\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution du script sur le master\n",
    "print(\"Patch des services Grafana et Prometheus en NodePort...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(PATCH_SERVICES_SCRIPT)\n",
    "\n",
    "# Affichage du r√©sultat\n",
    "print(\"\\n--- R√©sultat de la commande patch ---\")\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ef0ab",
   "metadata": {},
   "source": [
    "# Installer Kepler depuis les Manifestes Locaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- √âtape 1 : Copier le r√©pertoire local 'kepler' vers le master ---\n",
    "\n",
    "source_dir = \"kepler\"\n",
    "dest_dir = \"/tmp/kepler\"\n",
    "\n",
    "if not os.path.isdir(source_dir):\n",
    "    raise FileNotFoundError(f\"Le r√©pertoire local '{source_dir}' est introuvable.\")\n",
    "\n",
    "print(f\"--- Copie du r√©pertoire local '{source_dir}' vers '{dest_dir}' sur le master... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=source_dir, dest=\"/tmp/\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"R√©pertoire copi√© avec succ√®s.\")\n",
    "else:\n",
    "    print(\"Erreur lors de la copie du r√©pertoire.\")\n",
    "    if p.results:\n",
    "        print(p.results[0].stderr)\n",
    "\n",
    "\n",
    "# --- √âtape 2 : Appliquer les manifestes et v√©rifier ---\n",
    "\n",
    "# Script simplifi√© : la cr√©ation du namespace est g√©r√©e par les manifestes\n",
    "APPLY_KEPLER_MANIFESTS_SCRIPT = f\"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"--- Application de tous les manifestes depuis le r√©pertoire {dest_dir} ---\"\n",
    "kubectl apply -f \"{dest_dir}\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente de 30 secondes pour le d√©marrage des pods ---\"\n",
    "sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du statut des pods dans le namespace 'kepler' ---\"\n",
    "kubectl get pods -n kepler\n",
    "\n",
    "echo \"--- V√©rification que le Service a bien √©t√© cr√©√© ---\"\n",
    "kubectl get svc -n kepler -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"üéâ Kepler a √©t√© d√©ploy√© √† partir des manifestes locaux.\"\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n--- Application des manifestes sur le cluster... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(APPLY_KEPLER_MANIFESTS_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b551dc",
   "metadata": {},
   "source": [
    "## Diagnostic - Pourquoi Prometheus ne Scrape pas Kepler ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9602aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script de diagnostic complet pour identifier pourquoi Prometheus ne scrape pas Kepler\n",
    "DIAGNOSTIC_KEPLER_PROMETHEUS = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"======================================================================\"\n",
    "echo \"üîç DIAGNOSTIC COMPLET: Prometheus ‚Üî Kepler\"\n",
    "echo \"======================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 1. V√©rification des Pods Kepler ---\"\n",
    "kubectl get pods -n kepler -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 2. V√©rification du Service Kepler ---\"\n",
    "kubectl get svc -n kepler -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 3. V√©rification des Labels du Service Kepler ---\"\n",
    "echo \"Les labels sont CRITIQUES pour que le ServiceMonitor fonctionne!\"\n",
    "kubectl get svc -n kepler -o yaml | grep -A 10 \"labels:\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 4. V√©rification du ServiceMonitor Kepler ---\"\n",
    "kubectl get servicemonitor -n kepler -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 5. D√©tails du ServiceMonitor (selector et labels) ---\"\n",
    "kubectl get servicemonitor -n kepler -o yaml | grep -A 20 \"selector:\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 6. V√©rification que Prometheus-Operator voit le ServiceMonitor ---\"\n",
    "echo \"Prometheus doit √™tre dans le m√™me namespace ou avec les bonnes r√®gles RBAC\"\n",
    "kubectl get servicemonitor -A\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 7. Configuration de Prometheus pour ServiceMonitor ---\"\n",
    "echo \"V√©rification des serviceMonitorSelector dans Prometheus:\"\n",
    "kubectl get prometheus -n monitoring -o yaml | grep -A 5 \"serviceMonitorSelector:\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 8. V√©rification des Endpoints Kepler ---\"\n",
    "echo \"Si pas d'endpoints, le service ne pointe vers rien!\"\n",
    "kubectl get endpoints -n kepler\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 9. Test direct du endpoint Kepler ---\"\n",
    "KEPLER_POD=`kubectl get pods -n kepler -l app.kubernetes.io/name=kepler -o jsonpath='{.items[0].metadata.name}'`\n",
    "if [ -n \"$KEPLER_POD\" ]; then\n",
    "    echo \"Test de l'endpoint metrics sur le pod $KEPLER_POD:\"\n",
    "    kubectl exec -n kepler $KEPLER_POD -- curl -s localhost:9102/metrics | head -20\n",
    "else\n",
    "    echo \"‚ùå Aucun pod Kepler trouv√©!\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 10. V√©rification des Logs Prometheus ---\"\n",
    "echo \"Recherche d'erreurs li√©es √† Kepler dans les logs Prometheus:\"\n",
    "PROM_POD=`kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}'`\n",
    "if [ -n \"$PROM_POD\" ]; then\n",
    "    kubectl logs -n monitoring $PROM_POD -c prometheus --tail=50 | grep -i kepler || echo \"Aucune mention de Kepler dans les logs r√©cents\"\n",
    "else\n",
    "    echo \"‚ùå Pod Prometheus non trouv√©!\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 11. Configuration Actuelle des Targets Prometheus ---\"\n",
    "echo \"V√©rifiez si Kepler appara√Æt dans les targets de Prometheus\"\n",
    "echo \"üí° Acc√©dez √† l'UI Prometheus: Status ‚Üí Targets\"\n",
    "echo \"üí° Ou utilisez l'API: curl http://prometheus-ip:port/api/v1/targets\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"======================================================================\"\n",
    "echo \"üéØ CAUSES COMMUNES ET SOLUTIONS:\"\n",
    "echo \"======================================================================\"\n",
    "echo \"\"\n",
    "echo \"‚ùå PROBL√àME 1: Labels incompatibles\"\n",
    "echo \"   Le ServiceMonitor cherche des labels sp√©cifiques sur le Service\"\n",
    "echo \"   Solution: V√©rifiez que les labels du Service matchent le selector du ServiceMonitor\"\n",
    "echo \"\"\n",
    "echo \"‚ùå PROBL√àME 2: Namespace incorrect\"\n",
    "echo \"   Le ServiceMonitor doit √™tre dans un namespace que Prometheus surveille\"\n",
    "echo \"   Solution: Soit mettre le ServiceMonitor dans 'monitoring', soit configurer Prometheus\"\n",
    "echo \"\"\n",
    "echo \"‚ùå PROBL√àME 3: serviceMonitorSelector vide\"\n",
    "echo \"   Prometheus n'est configur√© pour surveiller AUCUN ServiceMonitor\"\n",
    "echo \"   Solution: Configurer prometheus.serviceMonitorSelector: {}\"\n",
    "echo \"\"\n",
    "echo \"‚ùå PROBL√àME 4: Port incorrect\"\n",
    "echo \"   Le ServiceMonitor pointe vers un port qui n'existe pas\"\n",
    "echo \"   Solution: V√©rifier que le port dans ServiceMonitor = port du Service\"\n",
    "echo \"\"\n",
    "echo \"======================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "# --- Enregistrer le script dans un fichier local ---\n",
    "script_path = \"diagnostic_kepler.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(DIAGNOSTIC_KEPLER_PROMETHEUS)\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(\"üîç Lancement du diagnostic complet Prometheus ‚Üî Kepler...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.script(script_path)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c1d10",
   "metadata": {},
   "source": [
    "## üîß Solution : Donner les Permissions RBAC √† Prometheus pour Kepler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3187c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour corriger les permissions RBAC de Prometheus\n",
    "FIX_PROMETHEUS_RBAC = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üîß Correction des Permissions RBAC pour Prometheus ‚Üí Kepler\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Cr√©ation d'un ClusterRole avec les permissions n√©cessaires ---\"\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRole\n",
    "metadata:\n",
    "  name: prometheus-kepler-access\n",
    "rules:\n",
    "- apiGroups: [\"\"]\n",
    "  resources:\n",
    "  - services\n",
    "  - endpoints\n",
    "  - pods\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "- apiGroups: [\"discovery.k8s.io\"]\n",
    "  resources:\n",
    "  - endpointslices\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Liaison du ClusterRole au ServiceAccount prometheus-k8s ---\"\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: prometheus-kepler-access\n",
    "roleRef:\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "  kind: ClusterRole\n",
    "  name: prometheus-kepler-access\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: prometheus-k8s\n",
    "  namespace: monitoring\n",
    "EOF\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des permissions cr√©√©es ---\"\n",
    "kubectl get clusterrole prometheus-kepler-access\n",
    "kubectl get clusterrolebinding prometheus-kepler-access\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Red√©marrage des pods Prometheus pour appliquer les changements ---\"\n",
    "kubectl rollout restart statefulset prometheus-k8s -n monitoring\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente du red√©marrage de Prometheus (30 secondes) ---\"\n",
    "sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification que Prometheus est pr√™t ---\"\n",
    "kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus -n monitoring --timeout=120s\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ Permissions RBAC configur√©es avec succ√®s !\"\n",
    "echo \"========================================================================\"\n",
    "echo \"\"\n",
    "echo \"Prometheus peut maintenant acc√©der aux EndpointSlices du namespace kepler.\"\n",
    "echo \"Attendez 1-2 minutes puis v√©rifiez les targets dans l'UI Prometheus.\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîß Application de la correction RBAC...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(FIX_PROMETHEUS_RBAC)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c02f4",
   "metadata": {},
   "source": [
    "# D√©ploiement des Ressources PowerCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Script pour d√©ployer tous les manifestes PowerCap ---\n",
    "\n",
    "source_dir = \"powercap\"\n",
    "dest_dir = \"/tmp/powercap\"\n",
    "\n",
    "# V√©rifier que le r√©pertoire local existe\n",
    "if not os.path.isdir(source_dir):\n",
    "    print(f\"‚ö†Ô∏è  Le r√©pertoire local '{source_dir}' n'existe pas.\")\n",
    "    print(f\"üìÅ Veuillez cr√©er le dossier '{source_dir}' avec les fichiers suivants:\")\n",
    "    print(\"   - daemonset.yaml\")\n",
    "    print(\"   - rbac.yaml\")\n",
    "    print(\"   - configmap.yaml\")\n",
    "    print(\"\\nüí° Ou uploadez le dossier complet dans le r√©pertoire de travail.\")\n",
    "    raise FileNotFoundError(f\"Le r√©pertoire local '{source_dir}' est introuvable. Cr√©ez-le d'abord avec les manifestes n√©cessaires.\")\n",
    "\n",
    "# --- √âtape 1 : Copier le r√©pertoire local vers le master ---\n",
    "print(f\"--- Copie du r√©pertoire '{source_dir}' vers '{dest_dir}' sur le master... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=source_dir, dest=\"/tmp/\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"‚úÖ R√©pertoire copi√© avec succ√®s.\")\n",
    "else:\n",
    "    print(\"‚ùå Erreur lors de la copie du r√©pertoire.\")\n",
    "    if p.results:\n",
    "        print(p.results[0].stderr)\n",
    "\n",
    "# --- √âtape 2 : Appliquer tous les manifestes ---\n",
    "DEPLOY_POWERCAP_SCRIPT = f\"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üöÄ D√©ploiement des Ressources PowerCap dans le namespace 'default'\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Application de tous les manifestes depuis {dest_dir} ---\"\n",
    "kubectl apply -f \"{dest_dir}\"\n",
    "\n",
    "# echo \"\"\n",
    "# echo \"--- Attente de 30 secondes pour la cr√©ation des ressources ---\"\n",
    "# sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du DaemonSet powercap-manager ---\"\n",
    "kubectl get daemonset powercap-manager -n default -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des pods powercap-manager ---\"\n",
    "kubectl get pods -n default -l app=powercap-manager -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Statut du DaemonSet (nombre de pods pr√™ts) ---\"\n",
    "kubectl rollout status daemonset/powercap-manager -n default --timeout=60s\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du ServiceAccount ---\"\n",
    "kubectl get serviceaccount powercap-manager -n default\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification de la ConfigMap ---\"\n",
    "kubectl get configmap powercap-config -n default\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des services (si pr√©sents) ---\"\n",
    "kubectl get services -n default -l app=powercap-manager || echo \"Aucun service trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ D√©ploiement PowerCap termin√© !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- D√©ploiement des ressources PowerCap... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DEPLOY_POWERCAP_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d700f",
   "metadata": {},
   "source": [
    "## V√©rification de l'√âtat et des Logs PowerCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d50d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour v√©rifier l'√©tat d√©taill√© et afficher les logs\n",
    "CHECK_POWERCAP_STATUS_LOGS = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üìä V√âRIFICATION D√âTAILL√âE - √âtat et Logs PowerCap Manager\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 1. √âtat du DaemonSet powercap-manager ---\"\n",
    "kubectl get daemonset powercap-manager -n default -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 2. D√©tails du DaemonSet ---\"\n",
    "kubectl describe daemonset powercap-manager -n default | head -50\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 3. Liste des pods powercap-manager sur tous les n≈ìuds ---\"\n",
    "kubectl get pods -n default -l app=powercap-manager -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 4. Nombre de pods actifs par rapport au nombre attendu ---\"\n",
    "DESIRED=$(kubectl get daemonset powercap-manager -n default -o jsonpath='{.status.desiredNumberScheduled}')\n",
    "READY=$(kubectl get daemonset powercap-manager -n default -o jsonpath='{.status.numberReady}')\n",
    "AVAILABLE=$(kubectl get daemonset powercap-manager -n default -o jsonpath='{.status.numberAvailable}')\n",
    "echo \"Pods d√©sir√©s: $DESIRED\"\n",
    "echo \"Pods pr√™ts: $READY\"\n",
    "echo \"Pods disponibles: $AVAILABLE\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 5. Description d√©taill√©e de chaque pod ---\"\n",
    "POWERCAP_PODS=$(kubectl get pods -n default -l app=powercap-manager -o jsonpath='{.items[*].metadata.name}')\n",
    "\n",
    "if [ -n \"$POWERCAP_PODS\" ]; then\n",
    "    for pod in $POWERCAP_PODS; do\n",
    "        echo \"\"\n",
    "        echo \"==========================================\"\n",
    "        echo \"Pod: $pod\"\n",
    "        echo \"==========================================\"\n",
    "        \n",
    "        echo \"\"\n",
    "        echo \"--- √âtat g√©n√©ral ---\"\n",
    "        kubectl get pod $pod -n default -o wide\n",
    "        \n",
    "        echo \"\"\n",
    "        echo \"--- N≈ìud d'ex√©cution ---\"\n",
    "        NODE=$(kubectl get pod $pod -n default -o jsonpath='{.spec.nodeName}')\n",
    "        echo \"N≈ìud: $NODE\"\n",
    "        \n",
    "        echo \"\"\n",
    "        echo \"--- Statut du conteneur ---\"\n",
    "        kubectl get pod $pod -n default -o jsonpath='{range .status.containerStatuses[*]}Conteneur: {.name}\n",
    "Ready: {.ready}\n",
    "Restarts: {.restartCount}\n",
    "Image: {.image}\n",
    "{end}'\n",
    "        \n",
    "        echo \"\"\n",
    "        echo \"--- Conditions du pod ---\"\n",
    "        kubectl get pod $pod -n default -o jsonpath='{range .status.conditions[*]}{.type}: {.status} ({.reason})\n",
    "{end}'\n",
    "    done\n",
    "else\n",
    "    echo \"‚ùå Aucun pod powercap-manager trouv√© dans le namespace default\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 6. Logs de tous les pods powercap-manager ---\"\n",
    "if [ -n \"$POWERCAP_PODS\" ]; then\n",
    "    for pod in $POWERCAP_PODS; do\n",
    "        echo \"\"\n",
    "        echo \"==========================================\"\n",
    "        echo \"üìù LOGS du pod: $pod\"\n",
    "        echo \"==========================================\"\n",
    "        kubectl logs $pod -n default --tail=100 2>&1 || echo \"Impossible de r√©cup√©rer les logs\"\n",
    "    done\n",
    "else\n",
    "    echo \"‚ùå Aucun log √† afficher\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 7. √âv√©nements r√©cents li√©s √† powercap-manager ---\"\n",
    "kubectl get events -n default --sort-by='.lastTimestamp' | grep powercap | tail -30 || echo \"Aucun √©v√©nement r√©cent\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 8. V√©rification de l'acc√®s aux ressources syst√®me ---\"\n",
    "if [ -n \"$POWERCAP_PODS\" ]; then\n",
    "    # Test sur le premier pod\n",
    "    FIRST_POD=$(echo $POWERCAP_PODS | awk '{print $1}')\n",
    "    echo \"Test d'acc√®s RAPL sur le pod: $FIRST_POD\"\n",
    "    kubectl exec $FIRST_POD -n default -- ls -la /sys/devices/virtual/powercap/ 2>&1 || echo \"‚ùå Impossible d'acc√©der au r√©pertoire RAPL\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 9. V√©rification de la ConfigMap ---\"\n",
    "kubectl get configmap powercap-config -n default -o yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 10. V√©rification des erreurs critiques ---\"\n",
    "if [ -n \"$POWERCAP_PODS\" ]; then\n",
    "    for pod in $POWERCAP_PODS; do\n",
    "        STATUS=$(kubectl get pod $pod -n default -o jsonpath='{.status.phase}')\n",
    "        if [ \"$STATUS\" != \"Running\" ]; then\n",
    "            echo \"\"\n",
    "            echo \"‚ö†Ô∏è  ATTENTION: Pod $pod est en √©tat: $STATUS\"\n",
    "            echo \"Raison:\"\n",
    "            kubectl get pod $pod -n default -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].message}'\n",
    "            echo \"\"\n",
    "            echo \"Logs d'erreur:\"\n",
    "            kubectl logs $pod -n default --tail=20 2>&1\n",
    "        fi\n",
    "    done\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ V√©rification termin√©e\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Lancement de la v√©rification d√©taill√©e de l'√©tat et des logs PowerCap...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(CHECK_POWERCAP_STATUS_LOGS)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9074831",
   "metadata": {},
   "source": [
    "## V√©rification des Annotations des N≈ìuds par PowerCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b89add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour v√©rifier les annotations ajout√©es par PowerCap sur les n≈ìuds\n",
    "CHECK_NODE_ANNOTATIONS_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üè∑Ô∏è  V√âRIFICATION DES ANNOTATIONS DES N≈íUDS\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Liste de tous les n≈ìuds du cluster ---\"\n",
    "kubectl get nodes\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Annotations compl√®tes de tous les n≈ìuds ---\"\n",
    "for node in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do\n",
    "    echo \"\"\n",
    "    echo \"==========================================\"\n",
    "    echo \"üìç N≈ìud: $node\"\n",
    "    echo \"==========================================\"\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"--- Type de n≈ìud ---\"\n",
    "    kubectl get node $node -o jsonpath='{.metadata.labels.node-role\\.kubernetes\\.io/control-plane}' > /dev/null 2>&1 && echo \"Type: Master/Control-Plane\" || echo \"Type: Worker\"\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"--- Toutes les annotations ---\"\n",
    "    kubectl get node $node -o jsonpath='{.metadata.annotations}' | jq '.' 2>/dev/null || kubectl get node $node -o jsonpath='{.metadata.annotations}'\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"--- Annotations sp√©cifiques √† PowerCap (filtr√©es) ---\"\n",
    "    kubectl get node $node -o json | jq -r '.metadata.annotations | to_entries[] | select(.key | contains(\"power\") or contains(\"rapl\") or contains(\"energy\") or contains(\"cap\")) | \"\\(.key): \\(.value)\"' 2>/dev/null || echo \"Aucune annotation PowerCap trouv√©e ou jq non disponible\"\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- R√©capitulatif : Annotations PowerCap par n≈ìud ---\"\n",
    "kubectl get nodes -o custom-columns=NAME:.metadata.name,ANNOTATIONS:.metadata.annotations 2>/dev/null || echo \"Format custom-columns non support√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ V√©rification des annotations termin√©e\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üè∑Ô∏è  V√©rification des annotations des n≈ìuds...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(CHECK_NODE_ANNOTATIONS_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ef55d",
   "metadata": {},
   "source": [
    "## Suppression du DaemonSet PowerCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85425d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# V√©rifier si le dossier local existe, sinon afficher une erreur\n",
    "source_dir = \"powercap\"\n",
    "dest_dir = \"/tmp/powercap\"\n",
    "\n",
    "if not os.path.isdir(source_dir):\n",
    "    print(f\"‚ùå Le r√©pertoire local '{source_dir}' n'existe pas.\")\n",
    "    print(\"üí° Cr√©ez le dossier avec les manifestes avant de supprimer les ressources.\")\n",
    "    raise FileNotFoundError(f\"Le r√©pertoire '{source_dir}' est introuvable.\")\n",
    "\n",
    "# --- Copier le r√©pertoire vers le master (au cas o√π il n'existe plus sur le master) ---\n",
    "print(f\"üìÅ V√©rification/copie du r√©pertoire '{source_dir}' vers le master...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=source_dir, dest=\"/tmp/\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"‚úÖ R√©pertoire copi√©/mis √† jour sur le master.\")\n",
    "\n",
    "# Script pour supprimer compl√®tement le DaemonSet PowerCap en utilisant les manifestes\n",
    "DELETE_POWERCAP_SCRIPT = f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üóëÔ∏è  SUPPRESSION des Ressources PowerCap\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Suppression de toutes les ressources via les manifestes ---\"\n",
    "kubectl delete -f {dest_dir}/ --ignore-not-found=true\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente de la suppression des pods (10 secondes) ---\"\n",
    "sleep 10\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Suppression des annotations PowerCap sur les n≈ìuds ---\"\n",
    "for node in $(kubectl get nodes -o jsonpath='{{.items[*].metadata.name}}'); do\n",
    "    echo \"Nettoyage des annotations sur le n≈ìud: $node\"\n",
    "    \n",
    "    # Suppression de chaque annotation PowerCap (avec 2>&1 pour voir les erreurs)\n",
    "    kubectl annotate node $node power-manager/initialized- 2>&1 || true\n",
    "    kubectl annotate node $node rapl/last-update- 2>&1 || true\n",
    "    kubectl annotate node $node rapl/market-period- 2>&1 || true\n",
    "    kubectl annotate node $node rapl/market-price- 2>&1 || true\n",
    "    kubectl annotate node $node rapl/market-volume- 2>&1 || true\n",
    "    kubectl annotate node $node rapl/max_power_uw- 2>&1 || true\n",
    "    kubectl annotate node $node rapl/pmax- 2>&1 || true\n",
    "    kubectl annotate node $node rapl/provider- 2>&1 || true\n",
    "done\n",
    "echo \"‚úÖ Annotations PowerCap supprim√©es de tous les n≈ìuds\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification finale ---\"\n",
    "echo \"\"\n",
    "echo \"DaemonSets restants avec 'powercap' :\"\n",
    "kubectl get daemonsets -n default | grep powercap || echo \"‚úÖ Aucun DaemonSet trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Pods restants avec 'powercap' :\"\n",
    "kubectl get pods -n default -l app=powercap-manager || echo \"‚úÖ Aucun pod trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"ConfigMaps restantes avec 'powercap' :\"\n",
    "kubectl get configmap -n default | grep powercap || echo \"‚úÖ Aucune ConfigMap trouv√©e\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"ServiceAccounts restants avec 'powercap' :\"\n",
    "kubectl get serviceaccount -n default | grep powercap || echo \"‚úÖ Aucun ServiceAccount trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"ClusterRoles restants avec 'powercap' :\"\n",
    "kubectl get clusterrole | grep powercap || echo \"‚úÖ Aucun ClusterRole trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"ClusterRoleBindings restants avec 'powercap' :\"\n",
    "kubectl get clusterrolebinding | grep powercap || echo \"‚úÖ Aucun ClusterRoleBinding trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des annotations restantes ---\"\n",
    "echo \"Annotations PowerCap/RAPL restantes sur les n≈ìuds:\"\n",
    "for node in $(kubectl get nodes -o jsonpath='{{.items[*].metadata.name}}'); do\n",
    "    echo \"\"\n",
    "    echo \"N≈ìud: $node\"\n",
    "    kubectl get node $node -o json | jq -r '.metadata.annotations | to_entries[] | select(.key | contains(\"power\") or contains(\"rapl\")) | \"  \\\\(.key): \\\\(.value)\"' 2>/dev/null || echo \"  Aucune annotation PowerCap/RAPL trouv√©e\"\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ Suppression de PowerCap termin√©e !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóëÔ∏è  Lancement de la suppression de PowerCap...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DELETE_POWERCAP_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)\n",
    "\n",
    "# --- Nettoyage des donn√©es sur les workers ---\n",
    "print(\"\\n--- Nettoyage des donn√©es sur les n≈ìuds workers ---\")\n",
    "CLEANUP_WORKERS_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "echo \"Suppression des donn√©es PowerCap dans /var/lib/powercap...\"\n",
    "sudo rm -rf /var/lib/powercap/*.csv 2>/dev/null || true\n",
    "echo \"‚úÖ Nettoyage termin√© sur ce n≈ìud\"\n",
    "\"\"\"\n",
    "\n",
    "with en.actions(roles=worker_nodes) as p:\n",
    "    p.shell(CLEANUP_WORKERS_SCRIPT)\n",
    "\n",
    "print(\"‚úÖ Toutes les ressources PowerCap ont √©t√© supprim√©es du cluster !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e2642",
   "metadata": {},
   "source": [
    "# D√©ploiement de Volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Script pour d√©ployer Volta ---\n",
    "\n",
    "source_dir = \"volta\"\n",
    "config_file = \"volta-config.yaml\"\n",
    "manifest_file = \"volta.yaml\"\n",
    "\n",
    "# V√©rifier que le r√©pertoire local existe\n",
    "if not os.path.isdir(source_dir):\n",
    "    print(f\"‚ùå Le r√©pertoire local '{source_dir}' n'existe pas.\")\n",
    "    print(f\"üìÅ Veuillez cr√©er le dossier '{source_dir}' avec les fichiers suivants:\")\n",
    "    print(f\"   - {config_file}\")\n",
    "    print(f\"   - {manifest_file}\")\n",
    "    print(\"\\nüí° Ou uploadez le dossier complet dans le r√©pertoire de travail.\")\n",
    "    raise FileNotFoundError(f\"Le r√©pertoire local '{source_dir}' est introuvable. Cr√©ez-le d'abord avec les manifestes n√©cessaires.\")\n",
    "\n",
    "# V√©rifier que les fichiers n√©cessaires existent\n",
    "config_path = os.path.join(source_dir, config_file)\n",
    "manifest_path = os.path.join(source_dir, manifest_file)\n",
    "\n",
    "if not os.path.isfile(config_path):\n",
    "    raise FileNotFoundError(f\"Le fichier de configuration '{config_path}' est introuvable.\")\n",
    "\n",
    "if not os.path.isfile(manifest_path):\n",
    "    raise FileNotFoundError(f\"Le fichier manifest '{manifest_path}' est introuvable.\")\n",
    "\n",
    "print(f\"‚úÖ Fichiers Volta trouv√©s:\")\n",
    "print(f\"   - Configuration: {config_path}\")\n",
    "print(f\"   - Manifest: {manifest_path}\")\n",
    "\n",
    "# --- √âtape 1 : Copier le fichier de configuration vers /etc/kubernetes/ sur le master ---\n",
    "print(f\"\\n--- Copie du fichier de configuration vers /etc/kubernetes/ sur le master... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=config_path, dest=\"/etc/kubernetes/volta-config.yaml\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"‚úÖ Fichier de configuration copi√© avec succ√®s.\")\n",
    "else:\n",
    "    print(\"‚ùå Erreur lors de la copie du fichier de configuration.\")\n",
    "    if p.results:\n",
    "        print(p.results[0].stderr)\n",
    "\n",
    "# --- √âtape 2 : Copier le manifest vers le master ---\n",
    "print(f\"\\n--- Copie du manifest Volta vers le master... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=manifest_path, dest=\"/tmp/volta.yaml\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"‚úÖ Manifest Volta copi√© avec succ√®s.\")\n",
    "else:\n",
    "    print(\"‚ùå Erreur lors de la copie du manifest.\")\n",
    "    if p.results:\n",
    "        print(p.results[0].stderr)\n",
    "\n",
    "# --- √âtape 3 : D√©ployer Volta ---\n",
    "DEPLOY_VOLTA_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"‚ö° D√©ploiement de Volta Scheduler\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du fichier de configuration ---\"\n",
    "if [ -f \"/etc/kubernetes/volta-config.yaml\" ]; then\n",
    "    echo \"‚úÖ Fichier de configuration trouv√©: /etc/kubernetes/volta-config.yaml\"\n",
    "    ls -la /etc/kubernetes/volta-config.yaml\n",
    "    echo \"\"\n",
    "    echo \"Contenu du fichier de configuration:\"\n",
    "    head -20 /etc/kubernetes/volta-config.yaml\n",
    "else\n",
    "    echo \"‚ùå Fichier de configuration non trouv√©!\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Application du manifest Volta ---\"\n",
    "kubectl apply -f /tmp/volta.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente de 30 secondes pour le d√©marrage ---\"\n",
    "sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du d√©ploiement volta-scheduler ---\"\n",
    "kubectl get deployment volta-scheduler -n kube-system -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des pods Volta (component=scheduler) ---\"\n",
    "kubectl get pods -n kube-system -l component=scheduler,tier=control-plane -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Statut du d√©ploiement Volta ---\"\n",
    "kubectl rollout status deployment/volta-scheduler -n kube-system --timeout=120s\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification de toutes les ressources li√©es √† Volta ---\"\n",
    "kubectl get all -n kube-system | grep volta || echo \"Aucune ressource Volta trouv√©e\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- √âv√©nements r√©cents li√©s √† volta-scheduler ---\"\n",
    "kubectl get events -n kube-system --sort-by='.lastTimestamp' | grep volta | tail -20 || echo \"Aucun √©v√©nement Volta r√©cent\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Logs des pods volta-scheduler ---\"\n",
    "kubectl logs -l component=scheduler,tier=control-plane -n kube-system --tail=30 2>&1 || echo \"Aucun log disponible\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ D√©ploiement de Volta Scheduler termin√© !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- D√©ploiement de Volta... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DEPLOY_VOLTA_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c0847",
   "metadata": {},
   "source": [
    "## R√©cup√©ration de Tous les Logs Volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8355e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour r√©cup√©rer tous les logs de Volta Scheduler\n",
    "GET_ALL_VOLTA_LOGS_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üìã R√âCUP√âRATION COMPL√àTE DES LOGS VOLTA SCHEDULER\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 1. Recherche de tous les pods volta-scheduler ---\"\n",
    "VOLTA_PODS=$(kubectl get pods -n kube-system -l component=scheduler,tier=control-plane -o jsonpath='{.items[*].metadata.name}' | grep volta)\n",
    "\n",
    "if [ -z \"$VOLTA_PODS\" ]; then\n",
    "    echo \"‚ùå Aucun pod volta-scheduler trouv√©\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"Pods Volta trouv√©s: $VOLTA_PODS\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 2. Informations d√©taill√©es sur les pods Volta ---\"\n",
    "for pod in $VOLTA_PODS; do\n",
    "    echo \"\"\n",
    "    echo \"==========================================\"\n",
    "    echo \"Pod: $pod\"\n",
    "    echo \"==========================================\"\n",
    "    \n",
    "    echo \"--- √âtat du pod ---\"\n",
    "    kubectl get pod $pod -n kube-system -o wide\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"--- N≈ìud d'ex√©cution ---\"\n",
    "    kubectl get pod $pod -n kube-system -o jsonpath='{.spec.nodeName}'\n",
    "    echo \"\"\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"--- Statut des conteneurs ---\"\n",
    "    kubectl get pod $pod -n kube-system -o jsonpath='{range .status.containerStatuses[*]}Conteneur: {.name}, Ready: {.ready}, Restarts: {.restartCount}{\"\\n\"}{end}'\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 3. LOGS COMPLETS DE TOUS LES PODS VOLTA ---\"\n",
    "for pod in $VOLTA_PODS; do\n",
    "    echo \"\"\n",
    "    echo \"==========================================\"\n",
    "    echo \"üìù LOGS COMPLETS du pod: $pod\"\n",
    "    echo \"==========================================\"\n",
    "    \n",
    "    # Logs depuis le d√©but (pas de --tail)\n",
    "    kubectl logs $pod -n kube-system --timestamps 2>&1 || echo \"‚ùå Impossible de r√©cup√©rer les logs\"\n",
    "    \n",
    "    echo \"\"\n",
    "    echo \"--- Logs du conteneur pr√©c√©dent (si red√©marrage) ---\"\n",
    "    kubectl logs $pod -n kube-system --previous --timestamps 2>/dev/null || echo \"Aucun log de conteneur pr√©c√©dent\"\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 4. LOGS EN TEMPS R√âEL (suivre les logs) ---\"\n",
    "echo \"üí° Pour suivre les logs en temps r√©el, utilisez:\"\n",
    "echo \"kubectl logs -f -l component=scheduler,tier=control-plane -n kube-system | grep volta\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 5. √âV√âNEMENTS LI√âS √Ä VOLTA ---\"\n",
    "kubectl get events -n kube-system --sort-by='.lastTimestamp' | grep -i volta | tail -50 || echo \"Aucun √©v√©nement Volta trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 6. DESCRIPTION D√âTAILL√âE DES PODS VOLTA ---\"\n",
    "for pod in $VOLTA_PODS; do\n",
    "    echo \"\"\n",
    "    echo \"==========================================\"\n",
    "    echo \"üìã DESCRIPTION du pod: $pod\"\n",
    "    echo \"==========================================\"\n",
    "    kubectl describe pod $pod -n kube-system\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 7. CONFIGURATION DU D√âPLOIEMENT VOLTA ---\"\n",
    "kubectl get deployment volta-scheduler -n kube-system -o yaml 2>&1 || echo \"D√©ploiement Volta non trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- 8. LOGS DE D√âBOGAGE - D√âCISIONS DE SCHEDULING ---\"\n",
    "echo \"Recherche des logs de scheduling dans tous les pods volta:\"\n",
    "for pod in $VOLTA_PODS; do\n",
    "    echo \"\"\n",
    "    echo \"--- D√©cisions de scheduling dans $pod ---\"\n",
    "    kubectl logs $pod -n kube-system | grep -i \"schedule\\|bind\\|filter\\|score\" | tail -20 || echo \"Aucun log de scheduling trouv√©\"\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ R√©cup√©ration compl√®te des logs Volta termin√©e\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã R√©cup√©ration de tous les logs Volta...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(GET_ALL_VOLTA_LOGS_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46effed1",
   "metadata": {},
   "source": [
    "## D√©ploiement d'Application Demo avec Volta Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Script pour d√©ployer l'application demo avec Volta Scheduler ---\n",
    "\n",
    "demo_dir = \"demo\"\n",
    "demo_file = \"rancher-demo.yaml\"\n",
    "\n",
    "# V√©rifier que le r√©pertoire et le fichier existent\n",
    "if not os.path.isdir(demo_dir):\n",
    "    print(f\"‚ùå Le r√©pertoire local '{demo_dir}' n'existe pas.\")\n",
    "    raise FileNotFoundError(f\"Le r√©pertoire '{demo_dir}' est introuvable.\")\n",
    "\n",
    "demo_path = os.path.join(demo_dir, demo_file)\n",
    "if not os.path.isfile(demo_path):\n",
    "    print(f\"‚ùå Le fichier '{demo_path}' n'existe pas.\")\n",
    "    raise FileNotFoundError(f\"Le fichier '{demo_path}' est introuvable.\")\n",
    "\n",
    "print(f\"‚úÖ Fichier demo trouv√©: {demo_path}\")\n",
    "\n",
    "\n",
    "# --- Copier le fichier demo vers le master ---\n",
    "print(f\"\\n--- Copie du fichier demo vers le master... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.copy(src=demo_path, dest=\"/tmp/rancher-demo.yaml\")\n",
    "\n",
    "if p.results and p.results[0].status == \"OK\":\n",
    "    print(\"‚úÖ Fichier demo copi√© avec succ√®s.\")\n",
    "else:\n",
    "    print(\"‚ùå Erreur lors de la copie du fichier demo.\")\n",
    "    if p.results:\n",
    "        print(p.results[0].stderr)\n",
    "\n",
    "# --- Appliquer le manifest demo avec Volta Scheduler ---\n",
    "DEPLOY_DEMO_WITH_VOLTA_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üöÄ D√©ploiement de l'Application Demo avec Volta Scheduler\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Application du manifest rancher-demo.yaml ---\"\n",
    "kubectl apply -f /tmp/rancher-demo.yaml\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente de 30 secondes pour le d√©marrage ---\"\n",
    "sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du d√©ploiement hello-world ---\"\n",
    "kubectl get deployment hello-world -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des pods hello-world ---\"\n",
    "kubectl get pods -l app=hello-world -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du scheduler utilis√© ---\"\n",
    "kubectl get pods -l app=hello-world -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.schedulerName}{\"\\t\"}{.spec.nodeName}{\"\\n\"}{end}' | column -t\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Statut du d√©ploiement ---\"\n",
    "kubectl rollout status deployment/hello-world --timeout=120s\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- √âv√©nements r√©cents li√©s √† hello-world ---\"\n",
    "kubectl get events --sort-by='.lastTimestamp' | grep hello-world | tail -10 || echo \"Aucun √©v√©nement r√©cent\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Logs du scheduler Volta (pour voir les d√©cisions de placement) ---\"\n",
    "kubectl logs -l component=scheduler,tier=control-plane -n kube-system --tail=20 | grep -i \"hello-world\\|volta\\|schedule\" || echo \"Aucun log de scheduling trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ D√©ploiement de l'application demo avec Volta termin√© !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- D√©ploiement de l'application demo avec Volta Scheduler... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DEPLOY_DEMO_WITH_VOLTA_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de65015",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_DEMO_WITH_VOLTA_SCRIPT = \"\"\"\n",
    "#!/bin/bash -e\n",
    "echo \"========================================================================\"\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du d√©ploiement hello-world ---\"\n",
    "kubectl get deployment hello-world -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des pods hello-world ---\"\n",
    "kubectl get pods -l app=hello-world -o wide\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification du scheduler utilis√© ---\"\n",
    "kubectl get pods -l app=hello-world -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.schedulerName}{\"\\t\"}{.spec.nodeName}{\"\\n\"}{end}' | column -t\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Statut du d√©ploiement ---\"\n",
    "kubectl rollout status deployment/hello-world --timeout=120s\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- √âv√©nements r√©cents li√©s √† hello-world ---\"\n",
    "kubectl get events --sort-by='.lastTimestamp' | grep hello-world | tail -10 || echo \"Aucun √©v√©nement r√©cent\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Logs du scheduler Volta (pour voir les d√©cisions de placement) ---\"\n",
    "kubectl logs -l component=scheduler,tier=control-plane -n kube-system --tail=20 | grep -i \"hello-world\\|volta\\|schedule\" || echo \"Aucun log de scheduling trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ D√©ploiement de l'application demo avec Volta termin√© !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- D√©ploiement de l'application demo avec Volta Scheduler... ---\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DEPLOY_DEMO_WITH_VOLTA_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d694a3",
   "metadata": {},
   "source": [
    "## Suppression de l'Application Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour supprimer l'application demo\n",
    "DELETE_DEMO_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üóëÔ∏è  SUPPRESSION de l'Application Demo\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Suppression du d√©ploiement hello-world ---\"\n",
    "kubectl delete deployment hello-world --ignore-not-found=true\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente de la suppression des pods (10 secondes) ---\"\n",
    "sleep 10\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Suppression des fichiers temporaires ---\"\n",
    "rm -f /tmp/rancher-demo.yaml /tmp/rancher-demo-volta.yaml 2>&1 || echo \"Fichiers temporaires d√©j√† supprim√©s\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification finale ---\"\n",
    "echo \"\"\n",
    "echo \"D√©ploiements restants avec 'hello-world' :\"\n",
    "kubectl get deployments | grep hello-world || echo \"‚úÖ Aucun d√©ploiement trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Pods restants avec 'hello-world' :\"\n",
    "kubectl get pods -l app=hello-world || echo \"‚úÖ Aucun pod trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ Suppression de l'application demo termin√©e !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóëÔ∏è  Lancement de la suppression de l'application demo...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DELETE_DEMO_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)\n",
    "\n",
    "print(\"‚úÖ L'application demo a √©t√© compl√®tement supprim√©e du cluster !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1eab8d",
   "metadata": {},
   "source": [
    "## Suppression de Volta Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour supprimer compl√®tement Volta Scheduler\n",
    "DELETE_VOLTA_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üóëÔ∏è  SUPPRESSION de Volta Scheduler\"\n",
    "echo \"========================================================================\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Suppression du d√©ploiement volta-scheduler ---\"\n",
    "kubectl delete deployment volta-scheduler -n kube-system --ignore-not-found=true\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Attente de la suppression des pods (10 secondes) ---\"\n",
    "sleep 10\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Suppression du fichier de configuration ---\"\n",
    "sudo rm -f /etc/kubernetes/volta-config.yaml 2>&1 || echo \"Fichier de configuration d√©j√† supprim√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Suppression du manifest temporaire ---\"\n",
    "rm -f /tmp/volta.yaml 2>&1 || echo \"Manifest temporaire d√©j√† supprim√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification finale ---\"\n",
    "echo \"\"\n",
    "echo \"D√©ploiements restants avec 'volta' :\"\n",
    "kubectl get deployments -n kube-system | grep volta || echo \"‚úÖ Aucun d√©ploiement trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Pods restants avec 'volta' :\"\n",
    "kubectl get pods -n kube-system | grep volta || echo \"‚úÖ Aucun pod trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Services restants avec 'volta' :\"\n",
    "kubectl get services -n kube-system | grep volta || echo \"‚úÖ Aucun service trouv√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des fichiers de configuration ---\"\n",
    "ls -la /etc/kubernetes/volta-config.yaml 2>/dev/null || echo \"‚úÖ Fichier de configuration supprim√©\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ Suppression de Volta Scheduler termin√©e !\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóëÔ∏è  Lancement de la suppression de Volta Scheduler...\")\n",
    "with en.actions(roles=master_node) as p:\n",
    "    p.shell(DELETE_VOLTA_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    output = p.results[0].stdout.strip()\n",
    "    print(output)\n",
    "\n",
    "print(\"‚úÖ Volta Scheduler a √©t√© compl√®tement supprim√© du cluster !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797649d",
   "metadata": {},
   "source": [
    "# Lib√©ration des Ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf720446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destruction de la r√©servation\n",
    "print(\"Lib√©ration des ressources sur Grid'5000...\")\n",
    "provider.destroy()\n",
    "print(\"Ressources lib√©r√©es. ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8edfca4",
   "metadata": {},
   "source": [
    "## üî• Script Stress-NG FINAL avec votre commande test√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28945c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script stress-ng SIMPLIFI√â - juste le stress sans monitoring\n",
    "SIMPLE_STRESS_SCRIPT = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# Installation de stress-ng si n√©cessaire\n",
    "if ! command -v stress-ng &> /dev/null; then\n",
    "    sudo apt-get update -qq\n",
    "    sudo apt-get install -y stress-ng\n",
    "fi\n",
    "\n",
    "# Lancement du stress\n",
    "stress-ng --cpu 52 \\\\\n",
    "          --cpu-method all \\\\\n",
    "          --vm 8 \\\\\n",
    "          --vm-bytes 80% \\\\\n",
    "          --vm-method all \\\\\n",
    "          --io 16 \\\\\n",
    "          --hdd 8 \\\\\n",
    "          --cache 52 \\\\\n",
    "          --cache-level 3 \\\\\n",
    "          --matrix 26 \\\\\n",
    "          --vecmath 26 \\\\\n",
    "          --timeout 600s \\\\\n",
    "          --metrics-brief\n",
    "\"\"\"\n",
    "\n",
    "print(\"üî• Lancement du stress-ng simplifi√©...\")\n",
    "print(\"‚è±Ô∏è  Dur√©e: 5 minutes\")\n",
    "print(\"\")\n",
    "\n",
    "# Lancement sur tous les workers\n",
    "with en.actions(roles=worker_nodes) as p:\n",
    "    p.shell(SIMPLE_STRESS_SCRIPT)\n",
    "\n",
    "if p.results:\n",
    "    for i, result in enumerate(p.results):\n",
    "        print(f\"WORKER {i+1} ({result.host}): \", end=\"\")\n",
    "        if result.status == \"OK\":\n",
    "            print(\"‚úÖ Stress termin√©\")\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690630fb",
   "metadata": {},
   "source": [
    "## üîç Examen des Limites de Puissance RAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour examiner les limites de puissance RAPL\n",
    "RAPL_LIMITS_SCRIPT = \"\"\"#!/bin/bash\n",
    "\n",
    "echo \"========================================================================\"\n",
    "echo \"üîç EXAMEN DES LIMITES DE PUISSANCE RAPL\"\n",
    "echo \"========================================================================\"\n",
    "echo \"Hostname: $(hostname)\"\n",
    "echo \"Date: $(date)\"\n",
    "echo \"\"\n",
    "\n",
    "# V√©rification de l'existence du r√©pertoire RAPL\n",
    "RAPL_DIR=\"/sys/devices/virtual/powercap/intel-rapl\"\n",
    "\n",
    "if [ ! -d \"$RAPL_DIR\" ]; then\n",
    "    echo \"‚ùå R√©pertoire RAPL non trouv√©: $RAPL_DIR\"\n",
    "    echo \"Intel RAPL non disponible sur ce syst√®me\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"‚úÖ R√©pertoire RAPL trouv√©: $RAPL_DIR\"\n",
    "echo \"\"\n",
    "\n",
    "echo \"--- Structure du r√©pertoire RAPL ---\"\n",
    "ls -la $RAPL_DIR/\n",
    "echo \"\"\n",
    "\n",
    "echo \"--- Recherche des fichiers *_power_limit_uw ---\"\n",
    "POWER_LIMIT_FILES=$(find $RAPL_DIR -name \"*_power_limit_uw\" 2>/dev/null)\n",
    "\n",
    "if [ -z \"$POWER_LIMIT_FILES\" ]; then\n",
    "    echo \"‚ùå Aucun fichier *_power_limit_uw trouv√©\"\n",
    "else\n",
    "    echo \"üìä Fichiers de limites de puissance trouv√©s:\"\n",
    "    echo \"$POWER_LIMIT_FILES\"\n",
    "    echo \"\"\n",
    "    \n",
    "    echo \"========================================================================\"\n",
    "    echo \"üìà VALEURS DES LIMITES DE PUISSANCE\"\n",
    "    echo \"========================================================================\"\n",
    "    \n",
    "    for file in $POWER_LIMIT_FILES; do\n",
    "        echo \"\"\n",
    "        echo \"--- Fichier: $file ---\"\n",
    "        \n",
    "        if [ -r \"$file\" ]; then\n",
    "            VALUE_UW=$(cat \"$file\" 2>/dev/null)\n",
    "            if [ -n \"$VALUE_UW\" ] && [ \"$VALUE_UW\" -ne 0 ] 2>/dev/null; then\n",
    "                # Conversion en Watts\n",
    "                VALUE_W=$(echo \"scale=2; $VALUE_UW / 1000000\" | bc 2>/dev/null || echo \"N/A\")\n",
    "                echo \"  Valeur brute (microWatts): $VALUE_UW\"\n",
    "                echo \"  Valeur en Watts: $VALUE_W W\"\n",
    "                \n",
    "                # D√©terminer le type de domaine\n",
    "                DOMAIN_NAME=$(basename $(dirname \"$file\"))\n",
    "                echo \"  Domaine: $DOMAIN_NAME\"\n",
    "                \n",
    "                # Afficher le nom du domaine si disponible\n",
    "                NAME_FILE=\"$(dirname \"$file\")/name\"\n",
    "                if [ -r \"$NAME_FILE\" ]; then\n",
    "                    DOMAIN_FRIENDLY_NAME=$(cat \"$NAME_FILE\" 2>/dev/null)\n",
    "                    echo \"  Nom du domaine: $DOMAIN_FRIENDLY_NAME\"\n",
    "                fi\n",
    "                \n",
    "            else\n",
    "                echo \"  ‚ö†Ô∏è  Valeur non disponible ou nulle\"\n",
    "            fi\n",
    "        else\n",
    "            echo \"  ‚ùå Fichier non accessible en lecture\"\n",
    "        fi\n",
    "    done\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- Informations additionnelles RAPL ---\"\n",
    "echo \"\"\n",
    "\n",
    "# Afficher tous les domaines RAPL\n",
    "echo \"üìã Tous les domaines RAPL disponibles:\"\n",
    "for domain_dir in $RAPL_DIR/intel-rapl:*; do\n",
    "    if [ -d \"$domain_dir\" ]; then\n",
    "        DOMAIN_NAME=$(basename \"$domain_dir\")\n",
    "        NAME_FILE=\"$domain_dir/name\"\n",
    "        if [ -r \"$NAME_FILE\" ]; then\n",
    "            FRIENDLY_NAME=$(cat \"$NAME_FILE\" 2>/dev/null)\n",
    "            echo \"  $DOMAIN_NAME: $FRIENDLY_NAME\"\n",
    "            \n",
    "            # Afficher la limite maximale si disponible\n",
    "            MAX_LIMIT_FILE=\"$domain_dir/max_power_range_uw\"\n",
    "            if [ -r \"$MAX_LIMIT_FILE\" ]; then\n",
    "                MAX_VALUE_UW=$(cat \"$MAX_LIMIT_FILE\" 2>/dev/null)\n",
    "                if [ -n \"$MAX_VALUE_UW\" ]; then\n",
    "                    MAX_VALUE_W=$(echo \"scale=2; $MAX_VALUE_UW / 1000000\" | bc 2>/dev/null || echo \"N/A\")\n",
    "                    echo \"    Limite maximale: $MAX_VALUE_W W\"\n",
    "                fi\n",
    "            fi\n",
    "        fi\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- V√©rification des permissions ---\"\n",
    "echo \"Permissions sur le r√©pertoire RAPL:\"\n",
    "ls -ld $RAPL_DIR\n",
    "echo \"\"\n",
    "echo \"Permissions sur les fichiers de limites:\"\n",
    "for file in $POWER_LIMIT_FILES; do\n",
    "    echo \"$(ls -l \"$file\")\"\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"========================================================================\"\n",
    "echo \"‚úÖ Examen des limites RAPL termin√©\"\n",
    "echo \"========================================================================\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Examen des limites de puissance RAPL sur les n≈ìuds WORKERS uniquement...\")\n",
    "print(\"üìä Recherche des fichiers *_power_limit_uw dans /sys/devices/virtual/powercap/intel-rapl\")\n",
    "print(\"\")\n",
    "\n",
    "# Cr√©er le script sur les workers uniquement et l'ex√©cuter\n",
    "with en.actions(roles=worker_nodes) as p:\n",
    "    # Cr√©er le fichier script\n",
    "    p.copy(content=RAPL_LIMITS_SCRIPT, dest=\"/tmp/check_rapl_limits.sh\", mode=\"0755\")\n",
    "\n",
    "with en.actions(roles=worker_nodes) as p:\n",
    "    # Ex√©cuter le script\n",
    "    p.shell(\"bash /tmp/check_rapl_limits.sh\")\n",
    "\n",
    "if p.results:\n",
    "    for i, result in enumerate(p.results):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìç WORKER {i+1}: {result.host}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        if result.status == \"OK\":\n",
    "            # Extraire les informations importantes\n",
    "            lines = result.stdout.split('\\n')\n",
    "            \n",
    "            # Afficher les limites de puissance trouv√©es\n",
    "            in_power_section = False\n",
    "            for line in lines:\n",
    "                if \"VALEURS DES LIMITES DE PUISSANCE\" in line:\n",
    "                    in_power_section = True\n",
    "                    print(\"üìà LIMITES DE PUISSANCE:\")\n",
    "                elif \"Examen des limites RAPL termin√©\" in line:\n",
    "                    in_power_section = False\n",
    "                elif in_power_section and (\"Valeur en Watts:\" in line or \"Domaine:\" in line or \"Nom du domaine:\" in line):\n",
    "                    print(f\"  {line.strip()}\")\n",
    "                elif \"Aucun fichier *_power_limit_uw trouv√©\" in line:\n",
    "                    print(\"‚ùå Aucune limite de puissance RAPL trouv√©e\")\n",
    "                elif \"R√©pertoire RAPL non trouv√©\" in line:\n",
    "                    print(\"‚ùå Intel RAPL non disponible\")\n",
    "            \n",
    "            # Afficher les domaines disponibles\n",
    "            print(\"\\nüìã Domaines RAPL:\")\n",
    "            for line in lines:\n",
    "                if \": \" in line and (\"package\" in line.lower() or \"core\" in line.lower() or \"dram\" in line.lower()):\n",
    "                    print(f\"  {line.strip()}\")\n",
    "                    \n",
    "        else:\n",
    "            print(f\"‚ùå Erreur: {result.stderr}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üí° INFO: Les limites RAPL permettent de contr√¥ler la consommation maximale\")\n",
    "print(\"üìä Ces valeurs sont utilis√©es par Kepler pour le monitoring √©nerg√©tique\")\n",
    "print(\"üñ•Ô∏è  Analyse effectu√©e uniquement sur les n≈ìuds workers\")\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
